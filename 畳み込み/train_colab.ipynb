{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒã‚¤ã‚ºæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆGoogle Colabç‰ˆï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€30ã‚¯ãƒ©ã‚¹åˆ†é¡ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“‹ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †\n",
    "\n",
    "1. **GPUã‚’æœ‰åŠ¹åŒ–**: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ â†’ GPUï¼ˆT4ï¼‰ã‚’é¸æŠ\n",
    "2. **Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ**ï¼ˆæ¨å¥¨ï¼‰:\n",
    "   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‚’Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "   - ã‚»ãƒ«1ã§Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
    "3. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™**:\n",
    "   - ã‚»ãƒ«2ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆï¼ˆ`baseline_dataset.pickle`ï¼‰\n",
    "   - ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—: `frequency_band`ï¼ˆå‘¨æ³¢æ•°å¸¯åŸŸé›†ä¸­ãƒã‚¤ã‚ºï¼‰\n",
    "   - åŒºé–“æ•°: 30åŒºé–“\n",
    "4. å„ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "\n",
    "## ğŸ“ ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ã«ã¤ã„ã¦\n",
    "\n",
    "ç¾åœ¨ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—:\n",
    "- **`frequency_band`**: å‘¨æ³¢æ•°å¸¯åŸŸé›†ä¸­ãƒã‚¤ã‚ºï¼ˆç‰¹å®šã®å‘¨æ³¢æ•°å¸¯åŸŸã«é›†ä¸­çš„ã«ç™ºç”Ÿï¼‰\n",
    "  - é›»æºãƒã‚¤ã‚ºã€å…±æŒ¯ã€ã‚¯ãƒ­ã‚¹ãƒˆãƒ¼ã‚¯ãªã©ã‚’æ¨¡æ“¬\n",
    "  - åŒºé–“å†…ã®ç‰¹å®šã®å‘¨æ³¢æ•°å¸¯åŸŸï¼ˆ30%ç¨‹åº¦ï¼‰ã«ã‚¬ã‚¦ã‚·ã‚¢ãƒ³åˆ†å¸ƒã§ãƒã‚¤ã‚ºãŒé›†ä¸­\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆï¼ˆæ¨å¥¨ï¼‰\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•ï¼‰\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/noise')  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„\n",
    "\n",
    "print(f\"ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}\")\n",
    "print(f\"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§: {os.listdir('.')[:10]}\")  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "!pip install torch torchvision scikit-learn matplotlib -q\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# matplotlibã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ•ã‚©ãƒ³ãƒˆè­¦å‘Šã‚’é˜²ããŸã‚ï¼‰\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree('/root/.cache/matplotlib', ignore_errors=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# matplotlibã®è­¦å‘Šã‚’æŠ‘åˆ¶\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ \n",
    "project_root = os.getcwd()\n",
    "sys.path.insert(0, project_root)\n",
    "sys.path.insert(0, os.path.join(project_root, 'ç•³ã¿è¾¼ã¿'))\n",
    "sys.path.insert(0, os.path.join(project_root, 'ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)'))\n",
    "\n",
    "# GPUã®ç¢ºèª\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUå: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆColabç”¨ï¼‰\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "# ç•³ã¿è¾¼ã¿/dataset.py ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ä¸­...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¨noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿½åŠ \n",
    "project_root = os.getcwd()\n",
    "noise_folder_path = os.path.join(project_root, 'ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)')\n",
    "noise_link_path = os.path.join(project_root, 'noise')\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ ï¼ˆdataset.pyã®ä¸­ã§project_rootã‹ã‚‰noiseã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ï¼‰\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)ãƒ•ã‚©ãƒ«ãƒ€ã‚’noiseã¨ã„ã†åå‰ã§ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ\n",
    "# ï¼ˆæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
    "if not os.path.exists(noise_link_path):\n",
    "    try:\n",
    "        os.symlink(noise_folder_path, noise_link_path)\n",
    "        print(f\"ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ: {noise_link_path} -> {noise_folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã®ä½œæˆã«å¤±æ•—: {e}\")\n",
    "        print(\"ä»£ã‚ã‚Šã«sys.modulesã«ç›´æ¥ç™»éŒ²ã—ã¾ã™...\")\n",
    "        # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ãŒä½œæˆã§ããªã„å ´åˆã®ä»£æ›¿æ–¹æ³•\n",
    "        import importlib.util\n",
    "        noise_spec = importlib.util.spec_from_file_location(\"noise\", os.path.join(noise_folder_path, \"__init__.py\"))\n",
    "        noise_module = importlib.util.module_from_spec(noise_spec)\n",
    "        sys.modules[\"noise\"] = noise_module\n",
    "        noise_spec.loader.exec_module(noise_module)\n",
    "        \n",
    "        add_noise_spec = importlib.util.spec_from_file_location(\"noise.add_noise\", os.path.join(noise_folder_path, \"add_noise.py\"))\n",
    "        add_noise_module = importlib.util.module_from_spec(add_noise_spec)\n",
    "        sys.modules[\"noise.add_noise\"] = add_noise_module\n",
    "        add_noise_spec.loader.exec_module(add_noise_module)\n",
    "else:\n",
    "    print(f\"ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™: {noise_link_path}\")\n",
    "\n",
    "# __file__ã‚’å®šç¾©ï¼ˆexec()ã§å®Ÿè¡Œã™ã‚‹å ´åˆã«å¿…è¦ï¼‰\n",
    "dataset_path = os.path.join(project_root, 'ç•³ã¿è¾¼ã¿', 'dataset.py')\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«åå‰ç©ºé–“ã«__file__ã‚’è¿½åŠ ã—ã¦å®Ÿè¡Œ\n",
    "# exec()ã®ç¬¬2å¼•æ•°ã«globals()ã‚’æ¸¡ã™ã¨ã€å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã®å¤‰æ•°ãŒç¾åœ¨ã®åå‰ç©ºé–“ã«åæ˜ ã•ã‚Œã‚‹\n",
    "exec_globals = globals()\n",
    "exec_globals['__file__'] = dataset_path\n",
    "\n",
    "# dataset.pyã‚’å®Ÿè¡Œ\n",
    "exec(open(dataset_path).read(), exec_globals)\n",
    "\n",
    "print(\"\\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†ï¼\")\n",
    "print(\"   baseline_dataset.pickle ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã¨è©•ä¾¡é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from ç•³ã¿è¾¼ã¿.model import SimpleCNN\n",
    "from eval import evaluate_baseline_model\n",
    "\n",
    "print(\"âœ“ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹\n",
    "class PSDDataset(Dataset):\n",
    "    \"\"\"PSDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"æ¤œè¨¼\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            \n",
    "            # NaN/Infãƒã‚§ãƒƒã‚¯\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "                print(f\"  NaN: {torch.isnan(outputs).sum().item()}, Inf: {torch.isinf(outputs).sum().item()}\")\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, all_predictions, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, labels in dataloader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # é †ä¼æ’­\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # NaN/Infãƒã‚§ãƒƒã‚¯\n",
    "        if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "            print(f\"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "            print(f\"  NaN: {torch.isnan(outputs).sum().item()}, Inf: {torch.isinf(outputs).sum().item()}\")\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # NaN/Infãƒã‚§ãƒƒã‚¯\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"è­¦å‘Š: æå¤±ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "            continue\n",
    "        \n",
    "        # é€†ä¼æ’­\n",
    "        loss.backward()\n",
    "        \n",
    "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºã‚’é˜²ãï¼‰\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # çµ±è¨ˆ\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å‰ã«å®šç¾©ï¼‰\n",
    "NUM_CLASSES = 30  # 30ã‚¯ãƒ©ã‚¹åˆ†é¡\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "with open('baseline_dataset.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_data = dataset['train']['data']\n",
    "train_labels = dataset['train']['labels']\n",
    "val_data = dataset['val']['data']\n",
    "val_labels = dataset['val']['labels']\n",
    "test_data = dataset['test']['data']\n",
    "test_labels = dataset['test']['labels']\n",
    "\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "train_data = torch.FloatTensor(train_data)\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "val_data = torch.FloatTensor(val_data)\n",
    "val_labels = torch.LongTensor(val_labels)\n",
    "test_data = torch.FloatTensor(test_data)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã®ç¢ºèªï¼ˆ30ã‚¯ãƒ©ã‚¹åˆ†é¡ç”¨ï¼‰\n",
    "print(f\"\\nãƒ©ãƒ™ãƒ«ã®ç¢ºèª:\")\n",
    "print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ç¯„å›²: {train_labels.min().item()} - {train_labels.max().item()}\")\n",
    "print(f\"  æœŸå¾…ã•ã‚Œã‚‹ç¯„å›²: 0 - {NUM_CLASSES-1}\")\n",
    "if train_labels.max().item() >= NUM_CLASSES:\n",
    "    raise ValueError(f\"ã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ™ãƒ«ã®æœ€å¤§å€¤({train_labels.max().item()})ãŒNUM_CLASSES({NUM_CLASSES})ä»¥ä¸Šã§ã™ã€‚\")\n",
    "print(\"âœ“ ãƒ©ãƒ™ãƒ«ãŒæ­£ã—ã„ç¯„å›²å†…ã§ã™\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ç¢ºèªï¼ˆdataset.pyã§æ—¢ã«å‰å‡¦ç†æ¸ˆã¿ï¼‰\n",
    "print(\"\\nãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ç¢ºèª:\")\n",
    "print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {train_data.mean():.6f}\")\n",
    "print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¨™æº–åå·®: {train_data.std():.6f}\")\n",
    "print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²: [{train_data.min():.6f}, {train_data.max():.6f}]\")\n",
    "print(\"âœ“ ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«å‰å‡¦ç†æ¸ˆã¿ã§ã™ï¼ˆdataset.pyã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° + ãƒ­ã‚°å¤‰æ› + æ­£è¦åŒ–æ¸ˆã¿ï¼‰\")\n",
    "print(\"âœ“ è¿½åŠ ã®å‰å‡¦ç†ã¯ä¸è¦ã§ã™\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "NUM_CLASSES = 30  # 30ã‚¯ãƒ©ã‚¹åˆ†é¡\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# DataLoaderã®ä½œæˆ\n",
    "train_dataset = PSDDataset(train_data, train_labels)\n",
    "val_dataset = PSDDataset(val_data, val_labels)\n",
    "test_dataset = PSDDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹é‡ã¿ã®è¨ˆç®—ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¯¾å¿œï¼‰\n",
    "# ã‚¯ãƒ©ã‚¹é‡ã¿ã®è¨ˆç®—ï¼ˆã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«å¯¾å¿œï¼‰\n",
    "print(\"\\nã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ä¸­...\")\n",
    "print(f\"ãƒ©ãƒ™ãƒ«ã®ç¯„å›²: {train_labels.min().item()} - {train_labels.max().item()}\")\n",
    "print(f\"NUM_CLASSES: {NUM_CLASSES}\")\n",
    "# ãƒ©ãƒ™ãƒ«ãŒNUM_CLASSESã‚’è¶…ãˆã¦ã„ãªã„ã‹ç¢ºèª\n",
    "if train_labels.max().item() >= NUM_CLASSES:\n",
    "    raise ValueError(f\"ã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ™ãƒ«ã®æœ€å¤§å€¤({train_labels.max().item()})ãŒNUM_CLASSES({NUM_CLASSES})ä»¥ä¸Šã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’10ã‚¯ãƒ©ã‚¹ç”¨ã«å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "# ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ï¼ˆç¢ºå®Ÿã«NUM_CLASSESè¦ç´ ã«ã™ã‚‹ï¼‰\n",
    "label_counts_full = torch.bincount(train_labels)\n",
    "label_counts = torch.zeros(NUM_CLASSES, dtype=torch.long)\n",
    "for i in range(min(NUM_CLASSES, len(label_counts_full))):\n",
    "    label_counts[i] = label_counts_full[i]\n",
    "# é‡ã¿ = ç·ã‚µãƒ³ãƒ—ãƒ«æ•° / (ã‚¯ãƒ©ã‚¹æ•° * å„ã‚¯ãƒ©ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«æ•°)\n",
    "class_weights = len(train_labels) / (NUM_CLASSES * (label_counts.float() + 1e-8))\n",
    "class_weights = class_weights / class_weights.sum() * NUM_CLASSES  # æ­£è¦åŒ–\n",
    "print(f\"ã‚¯ãƒ©ã‚¹é‡ã¿ã®å½¢çŠ¶: {class_weights.shape}\")\n",
    "print(f\"ã‚¯ãƒ©ã‚¹é‡ã¿ã®ç¯„å›²: [{class_weights.min():.3f}, {class_weights.max():.3f}]\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)  # é‡ã¿æ¸›è¡°ã‚’è¿½åŠ \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "print(\"\\nâœ“ ãƒ¢ãƒ‡ãƒ«ã¨å­¦ç¿’è¨­å®šã®æº–å‚™å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’å®Ÿè¡Œï¼ˆ100ã‚¨ãƒãƒƒã‚¯ï¼‰\n",
    "# å­¦ç¿’å±¥æ­´\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience = 10  # Early Stoppingã®patience\n",
    "patience_counter = 0  # Early Stoppingã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"å­¦ç¿’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {NUM_EPOCHS}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # æ¤œè¨¼\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # å­¦ç¿’ç‡ã®èª¿æ•´\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # å±¥æ­´ã®ä¿å­˜\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨Early Stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0  # ãƒ™ã‚¹ãƒˆç²¾åº¦æ›´æ–°æ™‚ã«ãƒªã‚»ãƒƒãƒˆ\n",
    "    else:\n",
    "        patience_counter += 1  # ãƒ™ã‚¹ãƒˆç²¾åº¦ãŒæ›´æ–°ã•ã‚Œãªã‹ã£ãŸ\n",
    "    \n",
    "    # Early Stoppingãƒã‚§ãƒƒã‚¯\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nâš  Early Stopping: {patience}ã‚¨ãƒãƒƒã‚¯é€£ç¶šã§æ”¹å–„ãªã—ã€‚å­¦ç¿’ã‚’åœæ­¢ã—ã¾ã™ã€‚\")\n",
    "        print(f\"   ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}% (ã‚¨ãƒãƒƒã‚¯ {epoch+1-patience})\")\n",
    "        break\n",
    "    \n",
    "    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ã”ã¨ã€ã¾ãŸã¯æœ€åˆã¨æœ€å¾Œï¼‰\n",
    "    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆæ¯ã‚¨ãƒãƒƒã‚¯ã€ã¾ãŸã¯æœ€åˆã¨æœ€å¾Œï¼‰\n",
    "    if (epoch + 1) % 1 == 0 or epoch == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_epochs = NUM_EPOCHS - (epoch + 1)\n",
    "        estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs if epoch > 0 else 0\n",
    "        \n",
    "        # å­¦ç¿’ç‡ã®ç¢ºèª\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ç¢ºèªï¼ˆæœ€åˆã®ã‚¨ãƒãƒƒã‚¯ã®ã¿ï¼‰\n",
    "        if epoch == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_data = train_data[:1].to(device)\n",
    "                sample_output = model(sample_data)\n",
    "                print(f\"\\n=== ã‚¨ãƒãƒƒã‚¯ {epoch+1} ã®è©³ç´° ===\")\n",
    "                print(f\"  å­¦ç¿’ç‡: {current_lr:.6f}\")\n",
    "                print(f\"  ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ›: {sample_output[0].tolist()}\")\n",
    "                print(f\"  äºˆæ¸¬ã‚¯ãƒ©ã‚¹: {sample_output[0].argmax().item()}\")\n",
    "                print(f\"  å‡ºåŠ›ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹: {torch.softmax(sample_output[0], dim=0).tolist()}\")\n",
    "            model.train()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}ç§’)\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  å­¦ç¿’ç‡: {current_lr:.6f}\")\n",
    "        if epoch > 0:\n",
    "            val_loss_change = val_losses[-1] - val_losses[-2] if len(val_losses) > 1 else 0\n",
    "            val_acc_change = val_accuracies[-1] - val_accuracies[-2] if len(val_accuracies) > 1 else 0\n",
    "            print(f\"  æ¤œè¨¼æå¤±ã®å¤‰åŒ–: {val_loss_change:+.4f}, æ¤œè¨¼ç²¾åº¦ã®å¤‰åŒ–: {val_acc_change:+.2f}%\")\n",
    "        if estimated_remaining > 0:\n",
    "            print(f\"  çµŒéæ™‚é–“: {elapsed_time/60:.1f}åˆ†, æ®‹ã‚Šè¦‹ç©ã‚‚ã‚Š: {estimated_remaining/60:.1f}åˆ†\")\n",
    "        print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬çµæœã‚’å–å¾—\n",
    "print(\"\\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬çµæœã‚’å–å¾—ä¸­...\")\n",
    "model.eval()\n",
    "all_test_predictions = []\n",
    "all_test_labels = []\n",
    "all_test_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_test_predictions.extend(predicted.cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "        all_test_outputs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "# äºˆæ¸¬çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"true_label\": all_test_labels,\n",
    "    \"predicted_label\": all_test_predictions,\n",
    "    \"correct\": [int(t == p) for t, p in zip(all_test_labels, all_test_predictions)]\n",
    "})\n",
    "\n",
    "# å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚‚è¿½åŠ \n",
    "for i in range(10):\n",
    "    results_df[f\"prob_class_{i}\"] = [probs[i] for probs in all_test_outputs]\n",
    "\n",
    "results_df.to_csv(\"prediction_results.csv\", index=False)\n",
    "print(\"äºˆæ¸¬çµæœã‚’ prediction_results.csv ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# æ··åŒè¡Œåˆ—ã‚‚ä¿å­˜\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(all_test_labels, all_test_predictions)\n",
    "cm_df = pd.DataFrame(cm, index=[f\"True_{i}\" for i in range(10)], columns=[f\"Pred_{i}\" for i in range(10)])\n",
    "cm_df.to_csv(\"confusion_matrix.csv\")\n",
    "print(\"æ··åŒè¡Œåˆ—ã‚’ confusion_matrix.csv ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º\n",
    "print(\"\\n=== äºˆæ¸¬çµæœã®çµ±è¨ˆ ===\")\n",
    "print(f\"æ­£è§£æ•°: {results_df['correct'].sum()} / {len(results_df)}\")\n",
    "print(f\"ç²¾åº¦: {results_df['correct'].mean()*100:.2f}%\")\n",
    "print(\"\\nã‚¯ãƒ©ã‚¹ã”ã¨ã®æ­£è§£æ•°:\")\n",
    "for i in range(10):\n",
    "    class_mask = results_df[\"true_label\"] == i\n",
    "    correct_count = results_df[class_mask][\"correct\"].sum()\n",
    "    total_count = class_mask.sum()\n",
    "    print(f\"  ã‚¯ãƒ©ã‚¹ {i}: {correct_count} / {total_count} ({correct_count/total_count*100:.1f}%)\")\n",
    "print(f\"å­¦ç¿’å®Œäº†ï¼ç·æ™‚é–“: {total_time/60:.1f}åˆ† ({total_time/3600:.2f}æ™‚é–“)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è©•ä¾¡\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}%\")\n",
    "\n",
    "# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’æ˜ç¤ºçš„ã«è¡Œã„ã€è­¦å‘Šã‚’æŠ‘åˆ¶\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# eval.pyã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡\n",
    "print(\"\\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...\")\n",
    "from eval import evaluate_baseline_model\n",
    "results = evaluate_baseline_model(model, test_loader, device=device, num_intervals=30)\n",
    "\n",
    "print(f\"\\n=== è©•ä¾¡çµæœ ===\")\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results['precision']:.4f}\")\n",
    "print(f\"  Recall: {results['recall']:.4f}\")\n",
    "print(f\"  F1-score: {results['f1_score']:.4f}\")\n",
    "print(f\"  Loss (CrossEntropyLoss): {results['loss']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–ï¼ˆè¨“ç·´æå¤±ã®ã¿ï¼‰\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "ax.plot(train_losses, label='Train Loss', color='blue')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Curve (Loss)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"å­¦ç¿’æ›²ç·šã‚’ 'baseline_training_curves.png' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'test_accuracy': test_acc,\n",
    "    'results': results\n",
    "}, 'baseline_model.pth')\n",
    "print(\"ãƒ¢ãƒ‡ãƒ«ã‚’ 'baseline_model.pth' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š\n",
    "- `baseline_model.pth` (å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«)\n",
    "- `baseline_confusion_matrix.png` (æ··åŒè¡Œåˆ—)\n",
    "- `baseline_training_curves.png` (å­¦ç¿’æ›²ç·š)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "from google.colab import files\n",
    "\n",
    "files.download('baseline_model.pth')\n",
    "files.download('baseline_confusion_matrix.png')\n",
    "files.download('baseline_training_curves.png')\n",
    "print(\"âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä»£æ›¿æ–¹æ³•: ã‚³ãƒ¼ãƒ‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ãŒä½¿ãˆãªã„å ´åˆï¼‰\n",
    "\n",
    "ä¸Šè¨˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ã€ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ï¼ˆä»£æ›¿æ–¹æ³•ï¼‰\n",
    "# âš ï¸ ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã®æ–¹ãŒé€Ÿã„ã®ã§ã€ãã¡ã‚‰ã‚’æ¨å¥¨ã—ã¾ã™\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„:\")\n",
    "print(\"1. baseline_dataset.pickle\")\n",
    "print(\"2. baseline_model.py\")\n",
    "print(\"3. evaluate_noise_detection.py\")\n",
    "print(\"\\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(\"\\nã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"  âœ“ {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.py ã¨ evaluate_noise_detection.py ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from baseline_model import SimpleCNN\n",
    "from evaluate_noise_detection import evaluate_baseline_model, plot_confusion_matrix\n",
    "\n",
    "print(\"âœ“ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10  # Google Colabã®GPUã§é«˜é€Ÿå®Ÿè¡Œ\n",
    "NUM_CLASSES = 30\n",
    "\n",
    "# DataLoaderã®ä½œæˆ\n",
    "train_dataset = PSDDataset(train_data, train_labels)\n",
    "val_dataset = PSDDataset(val_data, val_labels)\n",
    "test_dataset = PSDDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã‚’ç¢ºèª\n",
    "print(\"\\n=== ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ç¢ºèª ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ\n",
    "    dummy_input = torch.randn(1, 3000).to(device)\n",
    "    dummy_output = model(dummy_input)\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å…¥åŠ›ã®å½¢çŠ¶: {dummy_input.shape}\")\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®å½¢çŠ¶: {dummy_output.shape}\")\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®ç¯„å›²: [{dummy_output.min().item():.4f}, {dummy_output.max().item():.4f}]\")\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®å¹³å‡: {dummy_output.mean().item():.4f}\")\n",
    "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®æ¨™æº–åå·®: {dummy_output.std().item():.4f}\")\n",
    "    print(f\"  å„ã‚¯ãƒ©ã‚¹ã®å‡ºåŠ›: {dummy_output[0].tolist()}\")\n",
    "    print(f\"  ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹å¾Œã®ç¢ºç‡: {torch.softmax(dummy_output[0], dim=0).tolist()}\")\n",
    "model.train()\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’å®Ÿè¡Œ\n",
    "import time\n",
    "\n",
    "# å­¦ç¿’å±¥æ­´\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"å­¦ç¿’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {NUM_EPOCHS}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # æ¤œè¨¼\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # å­¦ç¿’ç‡ã®èª¿æ•´\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # å±¥æ­´ã®ä¿å­˜\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0  # ãƒ™ã‚¹ãƒˆç²¾åº¦æ›´æ–°æ™‚ã«ãƒªã‚»ãƒƒãƒˆ\n",
    "    else:\n",
    "        patience_counter += 1  # ãƒ™ã‚¹ãƒˆç²¾åº¦ãŒæ›´æ–°ã•ã‚Œãªã‹ã£ãŸ\n",
    "    \n",
    "    # Early Stoppingãƒã‚§ãƒƒã‚¯\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nâš  Early Stopping: {patience}ã‚¨ãƒãƒƒã‚¯é€£ç¶šã§æ”¹å–„ãªã—ã€‚å­¦ç¿’ã‚’åœæ­¢ã—ã¾ã™ã€‚\")\n",
    "        print(f\"   ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}% (ã‚¨ãƒãƒƒã‚¯ {epoch+1-patience})\")\n",
    "        break\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ã”ã¨ã€ã¾ãŸã¯æœ€åˆã¨æœ€å¾Œï¼‰\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_epochs = NUM_EPOCHS - (epoch + 1)\n",
    "        estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}ç§’)\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  çµŒéæ™‚é–“: {elapsed_time/60:.1f}åˆ†, æ®‹ã‚Šè¦‹ç©ã‚‚ã‚Š: {estimated_remaining/60:.1f}åˆ†\")\n",
    "        print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"å­¦ç¿’å®Œäº†ï¼ç·æ™‚é–“: {total_time/60:.1f}åˆ† ({total_time/3600:.2f}æ™‚é–“)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
