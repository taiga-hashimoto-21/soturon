{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ãƒã‚¤ã‚ºæ¤œå‡º + å¾©å…ƒãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆGoogle Colabç‰ˆï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ãƒã‚¤ã‚ºæ¤œå‡º + å¾©å…ƒã‚¿ã‚¹ã‚¯ã®å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“‹ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †\n",
    "\n",
    "1. **GPUã‚’æœ‰åŠ¹åŒ–**: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ â†’ GPUï¼ˆT4ï¼‰ã‚’é¸æŠ\n",
    "2. **Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ**ï¼ˆæ¨å¥¨ï¼‰:\n",
    "   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã‚’Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "   - ã‚»ãƒ«1ã§Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
    "3. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™**:\n",
    "   - ã‚»ãƒ«4ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆï¼ˆ`baseline_dataset.pickle`ï¼‰\n",
    "   - ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—: æ–°ã—ã„3ç¨®é¡ã®ãƒã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `power_supply`ï¼‰\n",
    "   - åŒºé–“æ•°: 30åŒºé–“\n",
    "4. å„ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "\n",
    "## ğŸ“ ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦\n",
    "\n",
    "**ãƒã‚¤ã‚ºæ¤œå‡º + å¾©å…ƒã‚¿ã‚¹ã‚¯**:\n",
    "1. **ãƒã‚¹ã‚¯äºˆæ¸¬**: ã©ã®åŒºé–“ã«ãƒã‚¤ã‚ºãŒã‚ã‚‹ã‹ã‚’äºˆæ¸¬ï¼ˆ30åŒºé–“ã®ãƒã‚¤ã‚ºå¼·åº¦ï¼‰\n",
    "2. **å¾©å…ƒ**: äºˆæ¸¬ã—ãŸåŒºé–“ã®ã¿ï¼ˆ100ãƒã‚¤ãƒ³ãƒˆï¼‰ã‚’å¾©å…ƒ\n",
    "3. **æå¤±**: ãƒã‚¹ã‚¯äºˆæ¸¬ãŒæ­£ã—ã„å ´åˆã®ã¿å¾©å…ƒæå¤±ã‚’è¨ˆç®—\n",
    "\n",
    "## ğŸ“ ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ã«ã¤ã„ã¦\n",
    "\n",
    "ç¾åœ¨ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ï¼ˆæ–°ã—ã„3ç¨®é¡ï¼‰:\n",
    "- **`power_supply`**: é›»æºãƒã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€2kHzã«ãƒ”ãƒ¼ã‚¯ï¼‰\n",
    "- **`interference`**: å¹²æ¸‰ãƒã‚¤ã‚ºï¼ˆ3kHzã«ãƒ”ãƒ¼ã‚¯ï¼‰\n",
    "- **`clock_leakage`**: ã‚¯ãƒ­ãƒƒã‚¯ãƒªãƒ¼ã‚¯ãƒã‚¤ã‚ºï¼ˆ5kHzã«ãƒ”ãƒ¼ã‚¯ï¼‰\n",
    "\n",
    "**è¨­å®šæ–¹æ³•**:\n",
    "- `dataset.py`ã®`NOISE_TYPE`ã§é¸æŠï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: `power_supply`ï¼‰\n",
    "- `USE_RANDOM_NOISE = True`ã«ã™ã‚‹ã¨3ç¨®é¡ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä½¿ç”¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆï¼ˆæ¨å¥¨ï¼‰\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•ï¼‰\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/soturon')  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹\n",
    "\n",
    "print(f\"ç¾åœ¨ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}\")\n",
    "print(f\"ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§: {os.listdir('.')[:10]}\")  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "!pip install torch torchvision scikit-learn matplotlib -q\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# matplotlibã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ•ã‚©ãƒ³ãƒˆè­¦å‘Šã‚’é˜²ããŸã‚ï¼‰\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree('/root/.cache/matplotlib', ignore_errors=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# matplotlibã®è­¦å‘Šã‚’æŠ‘åˆ¶\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ \n",
    "project_root = os.getcwd()\n",
    "sys.path.insert(0, project_root)\n",
    "sys.path.insert(0, os.path.join(project_root, 'ç•³ã¿è¾¼ã¿'))\n",
    "sys.path.insert(0, os.path.join(project_root, 'ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)'))\n",
    "\n",
    "# GPUã®ç¢ºèª\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUå: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆColabç”¨ï¼‰\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "# ç•³ã¿è¾¼ã¿/dataset.py ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ä¸­...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¨noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ã‚’è¿½åŠ \n",
    "project_root = os.getcwd()\n",
    "noise_folder_path = os.path.join(project_root, 'ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)')\n",
    "noise_link_path = os.path.join(project_root, 'noise')\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ ï¼ˆdataset.pyã®ä¸­ã§project_rootã‹ã‚‰noiseã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ï¼‰\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)ãƒ•ã‚©ãƒ«ãƒ€ã‚’noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ç™»éŒ²\n",
    "# æ³¨æ„: \"noise\"ãƒ•ã‚©ãƒ«ãƒ€ã¯ã€dataset.pyãŒã€Œfrom noise.add_noise import add_noise_to_intervalã€ã¨\n",
    "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªä»®æƒ³çš„ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åã§ã™ã€‚\n",
    "# å®Ÿéš›ã®ãƒ•ã‚©ãƒ«ãƒ€åã¯ã€Œãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)ã€ã§ã™ãŒã€Pythonã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ã¯ã€Œnoiseã€ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚\n",
    "print(\"noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’sys.modulesã«ç™»éŒ²ä¸­...\")\n",
    "\n",
    "# æ—¢å­˜ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’å‰Šé™¤ï¼ˆå¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰\n",
    "if os.path.exists(noise_link_path):\n",
    "    try:\n",
    "        if os.path.islink(noise_link_path):\n",
    "            os.unlink(noise_link_path)\n",
    "            print(f\"å¤ã„ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’å‰Šé™¤: {noise_link_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã®å‰Šé™¤ã«å¤±æ•—ï¼ˆç„¡è¦–ã—ã¾ã™ï¼‰: {e}\")\n",
    "\n",
    "# æ—¢å­˜ã®noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‰Šé™¤ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼‰\n",
    "if 'noise' in sys.modules:\n",
    "    del sys.modules['noise']\n",
    "if 'noise.add_noise' in sys.modules:\n",
    "    del sys.modules['noise.add_noise']\n",
    "\n",
    "# __pycache__ã‚’ã‚¯ãƒªã‚¢ï¼ˆå¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ï¼‰\n",
    "import shutil\n",
    "pycache_path = os.path.join(noise_folder_path, '__pycache__')\n",
    "if os.path.exists(pycache_path):\n",
    "    try:\n",
    "        shutil.rmtree(pycache_path)\n",
    "        print(f\"å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤: {pycache_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‰Šé™¤ã«å¤±æ•—ï¼ˆç„¡è¦–ã—ã¾ã™ï¼‰: {e}\")\n",
    "\n",
    "# __init__.pyã®å†…å®¹ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£\n",
    "init_path = os.path.join(noise_folder_path, '__init__.py')\n",
    "print(f\"  __init__.pyã®ãƒ‘ã‚¹: {init_path}\")\n",
    "print(f\"  ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹: {os.path.exists(init_path)}\")\n",
    "\n",
    "# Google Colabä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤ã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ­£ã—ã„å†…å®¹ã‚’æ›¸ãè¾¼ã‚€\n",
    "correct_init_content = '''\"\"\"\n",
    "æ¸¬å®šç³»ç”±æ¥ã®ãƒã‚¤ã‚ºã‚’ä»˜ä¸ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "\"\"\"\n",
    "\n",
    "from .power_supply_noise import add_power_supply_noise\n",
    "from .interference_noise import add_interference_noise\n",
    "from .clock_leakage_noise import add_clock_leakage_noise\n",
    "from .add_noise import add_noise_to_interval\n",
    "\n",
    "__all__ = [\n",
    "    'add_power_supply_noise',\n",
    "    'add_interference_noise',\n",
    "    'add_clock_leakage_noise',\n",
    "    'add_noise_to_interval'\n",
    "]\n",
    "'''\n",
    "\n",
    "# __init__.pyã®å†…å®¹ã‚’ç¢ºèª\n",
    "try:\n",
    "    with open(init_path, 'r', encoding='utf-8') as f:\n",
    "        current_content = f.read()\n",
    "    \n",
    "    # å¤ã„å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æ›¸ãæ›ãˆã‚‹\n",
    "    if 'frequency_band_noise' in current_content or 'localized_spike_noise' in current_content:\n",
    "        print(\"  å¤ã„__init__.pyã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æ­£ã—ã„å†…å®¹ã«æ›¸ãæ›ãˆã¾ã™...\")\n",
    "        with open(init_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(correct_init_content)\n",
    "        print(\"  âœ“ __init__.pyã‚’æ›´æ–°ã—ã¾ã—ãŸ\")\n",
    "    else:\n",
    "        print(\"  âœ“ __init__.pyã¯æ—¢ã«æ­£ã—ã„å†…å®¹ã§ã™\")\n",
    "except Exception as e:\n",
    "    print(f\"  âš  __init__.pyã®ç¢ºèªã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶šè¡Œã—ã¾ã™: {e}\")\n",
    "\n",
    "# ãƒã‚¤ã‚ºãƒ•ã‚©ãƒ«ãƒ€ã‚’sys.pathã«è¿½åŠ ï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è§£æ±ºã™ã‚‹ãŸã‚ï¼‰\n",
    "if noise_folder_path not in sys.path:\n",
    "    sys.path.insert(0, noise_folder_path)\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "# noiseãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®__init__.pyã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "init_spec = importlib.util.spec_from_file_location(\"noise\", init_path)\n",
    "init_module = importlib.util.module_from_spec(init_spec)\n",
    "sys.modules['noise'] = init_module\n",
    "# __file__ã¨__path__ã‚’è¨­å®šï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãŸã‚ï¼‰\n",
    "init_module.__file__ = init_path\n",
    "init_module.__path__ = [noise_folder_path]\n",
    "init_spec.loader.exec_module(init_module)\n",
    "\n",
    "# add_noise.pyã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "add_noise_path = os.path.join(noise_folder_path, 'add_noise.py')\n",
    "add_noise_spec = importlib.util.spec_from_file_location(\"noise.add_noise\", add_noise_path)\n",
    "add_noise_module = importlib.util.module_from_spec(add_noise_spec)\n",
    "sys.modules['noise.add_noise'] = add_noise_module\n",
    "add_noise_module.__file__ = add_noise_path\n",
    "add_noise_spec.loader.exec_module(add_noise_module)\n",
    "\n",
    "print(\"âœ“ noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’sys.modulesã«ç™»éŒ²ã—ã¾ã—ãŸ\")\n",
    "print(f\"  åˆ©ç”¨å¯èƒ½ãªé–¢æ•°: {init_module.__all__}\")\n",
    "\n",
    "# __file__ã‚’å®šç¾©ï¼ˆexec()ã§å®Ÿè¡Œã™ã‚‹å ´åˆã«å¿…è¦ï¼‰\n",
    "dataset_path = os.path.join(project_root, 'ç•³ã¿è¾¼ã¿', 'dataset.py')\n",
    "\n",
    "# dataset.pyã®å†…å®¹ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£\n",
    "print(\"\\ndataset.pyã®å†…å®¹ã‚’ç¢ºèªä¸­...\")\n",
    "try:\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        dataset_content = f.read()\n",
    "    \n",
    "    # å¤ã„ã‚³ãƒ¼ãƒ‰ï¼ˆlabel_counts[i]ã‚’ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¿®æ­£\n",
    "    if 'label_counts = torch.bincount(train_labels)' in dataset_content and \\\n",
    "       'for i in range(NUM_INTERVALS):' in dataset_content and \\\n",
    "       'label_counts[i].item()' in dataset_content and \\\n",
    "       'label_counts_full' not in dataset_content:\n",
    "        print(\"  å¤ã„dataset.pyã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æ­£ã—ã„å†…å®¹ã«æ›¸ãæ›ãˆã¾ã™...\")\n",
    "        \n",
    "        # ä¿®æ­£ç®‡æ‰€ã‚’ç‰¹å®šã—ã¦ç½®æ›\n",
    "        old_pattern = '''# ãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒã‚’ç¢ºèª\n",
    "print(\"\\\\nãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰:\")\n",
    "label_counts = torch.bincount(train_labels)\n",
    "for i in range(NUM_INTERVALS):\n",
    "    print(f\"  åŒºé–“ {i+1:2d}: {label_counts[i].item():4d}ã‚µãƒ³ãƒ—ãƒ«\")'''\n",
    "        \n",
    "        new_pattern = '''# ãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒã‚’ç¢ºèª\n",
    "print(\"\\\\nãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰:\")\n",
    "label_counts_full = torch.bincount(train_labels)\n",
    "# NUM_INTERVALSã®ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦æ‹¡å¼µï¼ˆå­˜åœ¨ã—ãªã„åŒºé–“ã¯0ï¼‰\n",
    "label_counts = torch.zeros(NUM_INTERVALS, dtype=torch.long)\n",
    "for i in range(min(NUM_INTERVALS, len(label_counts_full))):\n",
    "    label_counts[i] = label_counts_full[i]\n",
    "for i in range(NUM_INTERVALS):\n",
    "    print(f\"  åŒºé–“ {i+1:2d}: {label_counts[i].item():4d}ã‚µãƒ³ãƒ—ãƒ«\")'''\n",
    "        \n",
    "        if old_pattern in dataset_content:\n",
    "            dataset_content = dataset_content.replace(old_pattern, new_pattern)\n",
    "            with open(dataset_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(dataset_content)\n",
    "            print(\"  âœ“ dataset.pyã‚’æ›´æ–°ã—ã¾ã—ãŸ\")\n",
    "        else:\n",
    "            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå®Œå…¨ã«ä¸€è‡´ã—ãªã„å ´åˆã€ã‚ˆã‚ŠæŸ”è»Ÿãªç½®æ›ã‚’è©¦ã¿ã‚‹\n",
    "            import re\n",
    "            # label_counts = torch.bincount(train_labels)ã®ç›´å¾Œã«ä¿®æ­£ã‚’æŒ¿å…¥\n",
    "            pattern = r'(label_counts = torch\\.bincount\\(train_labels\\))\\s*(for i in range\\(NUM_INTERVALS\\):)'\n",
    "            replacement = r'''label_counts_full = torch.bincount(train_labels)\n",
    "# NUM_INTERVALSã®ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦æ‹¡å¼µï¼ˆå­˜åœ¨ã—ãªã„åŒºé–“ã¯0ï¼‰\n",
    "label_counts = torch.zeros(NUM_INTERVALS, dtype=torch.long)\n",
    "for i in range(min(NUM_INTERVALS, len(label_counts_full))):\n",
    "    label_counts[i] = label_counts_full[i]\n",
    "\\2'''\n",
    "            if re.search(pattern, dataset_content):\n",
    "                dataset_content = re.sub(pattern, replacement, dataset_content)\n",
    "                with open(dataset_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(dataset_content)\n",
    "                print(\"  âœ“ dataset.pyã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆæ­£è¦è¡¨ç¾ã§ä¿®æ­£ï¼‰\")\n",
    "            else:\n",
    "                print(\"  âš  dataset.pyã®ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "    else:\n",
    "        print(\"  âœ“ dataset.pyã¯æ—¢ã«æ­£ã—ã„å†…å®¹ã§ã™\")\n",
    "except Exception as e:\n",
    "    print(f\"  âš  dataset.pyã®ç¢ºèªã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶šè¡Œã—ã¾ã™: {e}\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«åå‰ç©ºé–“ã«__file__ã‚’è¿½åŠ ã—ã¦å®Ÿè¡Œ\n",
    "# exec()ã®ç¬¬2å¼•æ•°ã«globals()ã‚’æ¸¡ã™ã¨ã€å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã®å¤‰æ•°ãŒç¾åœ¨ã®åå‰ç©ºé–“ã«åæ˜ ã•ã‚Œã‚‹\n",
    "exec_globals = globals()\n",
    "exec_globals['__file__'] = dataset_path\n",
    "\n",
    "# dataset.pyã‚’å®Ÿè¡Œ\n",
    "exec(open(dataset_path).read(), exec_globals)\n",
    "\n",
    "print(\"\\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†ï¼\")\n",
    "print(\"   baseline_dataset.pickle ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã¨æå¤±é–¢æ•°ã€è©•ä¾¡é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from ç•³ã¿è¾¼ã¿.model import NoiseDetectionAndReconstructionModel\n",
    "from ç•³ã¿è¾¼ã¿.loss import compute_noise_detection_reconstruction_loss, create_true_mask_from_intervals\n",
    "from eval import evaluate_noise_detection_reconstruction_model\n",
    "\n",
    "print(\"âœ“ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n",
    "\n",
    "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ãŸå ´åˆï¼‰\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ãŸå¾Œã¯ã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "modules_to_reload = [\n",
    "    'ç•³ã¿è¾¼ã¿.model',\n",
    "    'ç•³ã¿è¾¼ã¿.loss',\n",
    "    'eval'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "        print(f\"âœ“ {module_name}ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå†èª­ã¿è¾¼ã¿å¾Œï¼‰\n",
    "from ç•³ã¿è¾¼ã¿.model import NoiseDetectionAndReconstructionModel\n",
    "from ç•³ã¿è¾¼ã¿.loss import compute_noise_detection_reconstruction_loss, create_true_mask_from_intervals\n",
    "from eval import evaluate_noise_detection_reconstruction_model\n",
    "\n",
    "print(\"\\nâœ… ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ï¼ˆãƒã‚¤ã‚ºæ¤œå‡º + å¾©å…ƒã‚¿ã‚¹ã‚¯ç”¨ï¼‰\n",
    "class NoiseDetectionReconstructionDataset(Dataset):\n",
    "    \"\"\"ãƒã‚¤ã‚ºæ¤œå‡º + å¾©å…ƒã‚¿ã‚¹ã‚¯ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, noisy_data, original_data, labels):\n",
    "        self.noisy_data = noisy_data  # ãƒã‚¤ã‚ºä»˜ããƒ‡ãƒ¼ã‚¿\n",
    "        self.original_data = original_data  # ãƒã‚¤ã‚ºä»˜ä¸å‰ã®ãƒ‡ãƒ¼ã‚¿\n",
    "        self.labels = labels  # ãƒã‚¤ã‚ºåŒºé–“ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.noisy_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'noisy_psd': self.noisy_data[idx],\n",
    "            'original_psd': self.original_data[idx],\n",
    "            'noise_intervals': self.labels[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, device, num_intervals=30, points_per_interval=100):\n",
    "    \"\"\"æ¤œè¨¼ï¼ˆãƒã‚¤ã‚ºæ¤œå‡º + å¾©å…ƒã‚¿ã‚¹ã‚¯ç”¨ï¼‰\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mask_loss = 0\n",
    "    total_reconstruction_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    from ç•³ã¿è¾¼ã¿.loss import create_true_mask_from_intervals\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            noisy_psd = batch['noisy_psd'].to(device)\n",
    "            original_psd = batch['original_psd'].to(device)\n",
    "            true_noise_intervals = batch['noise_intervals'].to(device)\n",
    "            \n",
    "            # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ï¼ˆ3ã¤ã®å€¤ã‚’è¿”ã™: mask, reconstructed_intervals, reconstructed_intervals_infoï¼‰\n",
    "            predicted_mask, reconstructed_interval, reconstructed_intervals_info = model(noisy_psd)\n",
    "            \n",
    "            # çœŸã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "            true_mask = create_true_mask_from_intervals(true_noise_intervals, num_intervals)\n",
    "            true_mask = true_mask.to(device)\n",
    "            \n",
    "            # æå¤±ã‚’è¨ˆç®—\n",
    "            loss, mask_loss, reconstruction_loss, loss_dict = \\\n",
    "                compute_noise_detection_reconstruction_loss(\n",
    "                    predicted_mask=predicted_mask,\n",
    "                    reconstructed_interval=reconstructed_interval,\n",
    "                    reconstructed_intervals_info=reconstructed_intervals_info,\n",
    "                    true_mask=true_mask,\n",
    "                    original_psd=original_psd,\n",
    "                    noisy_psd=noisy_psd,\n",
    "                    true_noise_intervals=true_noise_intervals,\n",
    "                    num_intervals=num_intervals,\n",
    "                    mask_loss_weight=0.5,\n",
    "                    reconstruction_loss_weight=1.0,\n",
    "                    points_per_interval=points_per_interval\n",
    "                )\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mask_loss += mask_loss.item()\n",
    "            total_reconstruction_loss += reconstruction_loss.item()\n",
    "            \n",
    "            # ãƒã‚¹ã‚¯äºˆæ¸¬ã®ç²¾åº¦\n",
    "            predicted_intervals = predicted_mask.argmax(dim=1)\n",
    "            total += true_noise_intervals.size(0)\n",
    "            correct += (predicted_intervals == true_noise_intervals).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_mask_loss = total_mask_loss / len(dataloader)\n",
    "    avg_reconstruction_loss = total_reconstruction_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, avg_mask_loss, avg_reconstruction_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, num_intervals=30, points_per_interval=100):\n",
    "    \"\"\"1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’ï¼ˆãƒã‚¤ã‚ºæ¤œå‡º + å¾©å…ƒã‚¿ã‚¹ã‚¯ç”¨ï¼‰\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mask_loss = 0\n",
    "    total_reconstruction_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    from ç•³ã¿è¾¼ã¿.loss import create_true_mask_from_intervals\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        noisy_psd = batch['noisy_psd'].to(device)\n",
    "        original_psd = batch['original_psd'].to(device)\n",
    "        true_noise_intervals = batch['noise_intervals'].to(device)\n",
    "        \n",
    "        # é †ä¼æ’­\n",
    "        optimizer.zero_grad()\n",
    "        predicted_mask, reconstructed_interval, reconstructed_intervals_info = model(noisy_psd)\n",
    "        \n",
    "        # NaN/Infãƒã‚§ãƒƒã‚¯\n",
    "        if torch.isnan(predicted_mask).any() or torch.isinf(predicted_mask).any():\n",
    "            print(f\"è­¦å‘Š: ãƒã‚¹ã‚¯äºˆæ¸¬ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "            continue\n",
    "        \n",
    "        if torch.isnan(reconstructed_interval).any() or torch.isinf(reconstructed_interval).any():\n",
    "            print(f\"è­¦å‘Š: å¾©å…ƒå‡ºåŠ›ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "            continue\n",
    "        \n",
    "        # çœŸã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "        true_mask = create_true_mask_from_intervals(true_noise_intervals, num_intervals)\n",
    "        true_mask = true_mask.to(device)\n",
    "        \n",
    "        # æå¤±ã‚’è¨ˆç®—\n",
    "        loss, mask_loss, reconstruction_loss, loss_dict = \\\n",
    "            compute_noise_detection_reconstruction_loss(\n",
    "                predicted_mask=predicted_mask,\n",
    "                reconstructed_interval=reconstructed_interval,\n",
    "                reconstructed_intervals_info=reconstructed_intervals_info,\n",
    "                true_mask=true_mask,\n",
    "                original_psd=original_psd,\n",
    "                noisy_psd=noisy_psd,\n",
    "                true_noise_intervals=true_noise_intervals,\n",
    "                num_intervals=num_intervals,\n",
    "                mask_loss_weight=0.5,\n",
    "                reconstruction_loss_weight=1.0,\n",
    "                points_per_interval=points_per_interval\n",
    "            )\n",
    "        \n",
    "        # NaN/Infãƒã‚§ãƒƒã‚¯\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"è­¦å‘Š: æå¤±ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ\")\n",
    "            continue\n",
    "        \n",
    "        # é€†ä¼æ’­\n",
    "        loss.backward()\n",
    "        \n",
    "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºã‚’é˜²ãï¼‰\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # çµ±è¨ˆ\n",
    "        total_loss += loss.item()\n",
    "        total_mask_loss += mask_loss.item()\n",
    "        total_reconstruction_loss += reconstruction_loss.item()\n",
    "        \n",
    "        # ãƒã‚¹ã‚¯äºˆæ¸¬ã®ç²¾åº¦\n",
    "        predicted_intervals = predicted_mask.argmax(dim=1)\n",
    "        total += true_noise_intervals.size(0)\n",
    "        correct += (predicted_intervals == true_noise_intervals).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_mask_loss = total_mask_loss / len(dataloader)\n",
    "    avg_reconstruction_loss = total_reconstruction_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, avg_mask_loss, avg_reconstruction_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "NUM_INTERVALS = 30  # 30åŒºé–“\n",
    "POINTS_PER_INTERVAL = 100  # 1åŒºé–“ã‚ãŸã‚Š100ãƒã‚¤ãƒ³ãƒˆ\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "with open('baseline_dataset.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_noisy = dataset['train']['noisy_data']\n",
    "train_original = dataset['train']['original_data']\n",
    "train_labels = dataset['train']['labels']\n",
    "val_noisy = dataset['val']['noisy_data']\n",
    "val_original = dataset['val']['original_data']\n",
    "val_labels = dataset['val']['labels']\n",
    "test_noisy = dataset['test']['noisy_data']\n",
    "test_original = dataset['test']['original_data']\n",
    "test_labels = dataset['test']['labels']\n",
    "\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_noisy):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_noisy):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_noisy):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "train_noisy = torch.FloatTensor(train_noisy)\n",
    "train_original = torch.FloatTensor(train_original)\n",
    "train_labels = torch.LongTensor(train_labels)\n",
    "val_noisy = torch.FloatTensor(val_noisy)\n",
    "val_original = torch.FloatTensor(val_original)\n",
    "val_labels = torch.LongTensor(val_labels)\n",
    "test_noisy = torch.FloatTensor(test_noisy)\n",
    "test_original = torch.FloatTensor(test_original)\n",
    "test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "# ãƒ©ãƒ™ãƒ«ã®ç¢ºèª\n",
    "print(f\"\\nãƒ©ãƒ™ãƒ«ã®ç¢ºèª:\")\n",
    "print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ç¯„å›²: {train_labels.min().item()} - {train_labels.max().item()}\")\n",
    "print(f\"  æœŸå¾…ã•ã‚Œã‚‹ç¯„å›²: 0 - {NUM_INTERVALS-1}\")\n",
    "if train_labels.max().item() >= NUM_INTERVALS:\n",
    "    raise ValueError(f\"ã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ™ãƒ«ã®æœ€å¤§å€¤({train_labels.max().item()})ãŒNUM_INTERVALS({NUM_INTERVALS})ä»¥ä¸Šã§ã™ã€‚\")\n",
    "print(\"âœ“ ãƒ©ãƒ™ãƒ«ãŒæ­£ã—ã„ç¯„å›²å†…ã§ã™\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ç¢ºèªï¼ˆdataset.pyã§æ—¢ã«å‰å‡¦ç†æ¸ˆã¿ï¼‰\n",
    "print(\"\\nãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ç¢ºèª:\")\n",
    "print(f\"  ãƒã‚¤ã‚ºä»˜ããƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {train_noisy.mean():.6f}, æ¨™æº–åå·®: {train_noisy.std():.6f}\")\n",
    "print(f\"  å…ƒã®ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡: {train_original.mean():.6f}, æ¨™æº–åå·®: {train_original.std():.6f}\")\n",
    "print(\"âœ“ ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«å‰å‡¦ç†æ¸ˆã¿ã§ã™ï¼ˆdataset.pyã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° + ãƒ­ã‚°å¤‰æ› + æ­£è¦åŒ–æ¸ˆã¿ï¼‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 100\n",
    "NUM_INTERVALS = 30\n",
    "POINTS_PER_INTERVAL = 100\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "model = NoiseDetectionAndReconstructionModel(\n",
    "    num_intervals=NUM_INTERVALS,\n",
    "    points_per_interval=POINTS_PER_INTERVAL\n",
    ").to(device)\n",
    "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# DataLoaderã®ä½œæˆ\n",
    "train_dataset = NoiseDetectionReconstructionDataset(train_noisy, train_original, train_labels)\n",
    "val_dataset = NoiseDetectionReconstructionDataset(val_noisy, val_original, val_labels)\n",
    "test_dataset = NoiseDetectionReconstructionDataset(test_noisy, test_original, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "print(\"\\nâœ“ ãƒ¢ãƒ‡ãƒ«ã¨å­¦ç¿’è¨­å®šã®æº–å‚™å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ï¼ˆå­¦ç¿’å‰ã«æœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºå®Ÿã«èª­ã¿è¾¼ã‚€ï¼‰\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ãŸå¾Œã¯ã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "modules_to_reload = [\n",
    "    'ç•³ã¿è¾¼ã¿.model',\n",
    "    'ç•³ã¿è¾¼ã¿.loss',\n",
    "    'eval'\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "        print(f\"âœ“ {module_name}ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ\")\n",
    "    else:\n",
    "        print(f\"âš  {module_name}ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã¾ã èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "\n",
    "# å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå†èª­ã¿è¾¼ã¿å¾Œï¼‰\n",
    "from ç•³ã¿è¾¼ã¿.model import NoiseDetectionAndReconstructionModel\n",
    "from ç•³ã¿è¾¼ã¿.loss import compute_noise_detection_reconstruction_loss, create_true_mask_from_intervals\n",
    "from eval import evaluate_noise_detection_reconstruction_model\n",
    "\n",
    "print(\"\\nâœ… ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’å®Ÿè¡Œ\n",
    "# å­¦ç¿’å±¥æ­´\n",
    "train_losses = []\n",
    "train_mask_losses = []\n",
    "train_reconstruction_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_mask_losses = []\n",
    "val_reconstruction_losses = []\n",
    "val_accuracies = []\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience = 10  # Early Stoppingã®patience\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"å­¦ç¿’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {NUM_EPOCHS}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    train_loss, train_mask_loss, train_reconstruction_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, device, NUM_INTERVALS, POINTS_PER_INTERVAL\n",
    "    )\n",
    "    \n",
    "    # æ¤œè¨¼\n",
    "    val_loss, val_mask_loss, val_reconstruction_loss, val_acc = validate(\n",
    "        model, val_loader, device, NUM_INTERVALS, POINTS_PER_INTERVAL\n",
    "    )\n",
    "    \n",
    "    # å­¦ç¿’ç‡ã®èª¿æ•´\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # å±¥æ­´ã®ä¿å­˜\n",
    "    train_losses.append(train_loss)\n",
    "    train_mask_losses.append(train_mask_loss)\n",
    "    train_reconstruction_losses.append(train_reconstruction_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_mask_losses.append(val_mask_loss)\n",
    "    val_reconstruction_losses.append(val_reconstruction_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨Early Stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early Stoppingãƒã‚§ãƒƒã‚¯\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nâš  Early Stopping: {patience}ã‚¨ãƒãƒƒã‚¯é€£ç¶šã§æ”¹å–„ãªã—ã€‚å­¦ç¿’ã‚’åœæ­¢ã—ã¾ã™ã€‚\")\n",
    "        print(f\"   ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}% (ã‚¨ãƒãƒƒã‚¯ {epoch+1-patience})\")\n",
    "        break\n",
    "    \n",
    "    # ãƒ­ã‚°å‡ºåŠ›\n",
    "    if (epoch + 1) % 1 == 0 or epoch == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_epochs = NUM_EPOCHS - (epoch + 1)\n",
    "        estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs if epoch > 0 else 0\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ç¢ºèªï¼ˆæœ€åˆã®ã‚¨ãƒãƒƒã‚¯ã®ã¿ï¼‰\n",
    "        if epoch == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_noisy = train_noisy[:1].to(device)\n",
    "                sample_mask, sample_reconstructed, sample_reconstructed_info = model(sample_noisy)\n",
    "                print(f\"\\n=== ã‚¨ãƒãƒƒã‚¯ {epoch+1} ã®è©³ç´° ===\")\n",
    "                print(f\"  å­¦ç¿’ç‡: {current_lr:.6f}\")\n",
    "                print(f\"  ã‚µãƒ³ãƒ—ãƒ«ãƒã‚¹ã‚¯äºˆæ¸¬: {sample_mask[0].argmax().item()} (ç¢ºä¿¡åº¦: {sample_mask[0].max().item():.4f})\")\n",
    "                print(f\"  å¾©å…ƒåŒºé–“ã®å½¢çŠ¶: {sample_reconstructed.shape}\")\n",
    "                print(f\"  å¾©å…ƒåŒºé–“æƒ…å ±: start={sample_reconstructed_info[0, 0].item()}, end={sample_reconstructed_info[0, 1].item()}, predicted={sample_reconstructed_info[0, 2].item()}\")\n",
    "            model.train()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}ç§’)\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} (Mask: {train_mask_loss:.4f}, Recon: {train_reconstruction_loss:.4f}), Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} (Mask: {val_mask_loss:.4f}, Recon: {val_reconstruction_loss:.4f}), Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  å­¦ç¿’ç‡: {current_lr:.6f}\")\n",
    "        if epoch > 0:\n",
    "            val_loss_change = val_losses[-1] - val_losses[-2] if len(val_losses) > 1 else 0\n",
    "            val_acc_change = val_accuracies[-1] - val_accuracies[-2] if len(val_accuracies) > 1 else 0\n",
    "            print(f\"  æ¤œè¨¼æå¤±ã®å¤‰åŒ–: {val_loss_change:+.4f}, æ¤œè¨¼ç²¾åº¦ã®å¤‰åŒ–: {val_acc_change:+.2f}%\")\n",
    "        if estimated_remaining > 0:\n",
    "            print(f\"  çµŒéæ™‚é–“: {elapsed_time/60:.1f}åˆ†, æ®‹ã‚Šè¦‹ç©ã‚‚ã‚Š: {estimated_remaining/60:.1f}åˆ†\")\n",
    "        print()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"å­¦ç¿’å®Œäº†ï¼ç·æ™‚é–“: {total_time/60:.1f}åˆ† ({total_time/3600:.2f}æ™‚é–“)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# è©•ä¾¡ç”¨ã‚»ãƒ«ï¼ˆç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½ï¼‰\n",
    "# å¤‰æ•°ãŒæ¶ˆãˆã¦ã‚‚ã€ã“ã®ã‚»ãƒ«ã ã‘å®Ÿè¡Œã™ã‚Œã°è©•ä¾¡ã§ãã¾ã™\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ãƒ‘ã‚¹è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰\n",
    "project_root = '/content/drive/MyDrive/soturon'\n",
    "pickle_path = f'{project_root}/baseline_dataset.pickle'\n",
    "model_path = f'{project_root}/ç•³ã¿è¾¼ã¿/noise_detection_reconstruction_model.pth'\n",
    "\n",
    "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ï¼ˆæœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºå®Ÿã«èª­ã¿è¾¼ã‚€ï¼‰\n",
    "modules_to_reload = ['ç•³ã¿è¾¼ã¿.model', 'ç•³ã¿è¾¼ã¿.loss', 'eval']\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "        print(f\"âœ“ {module_name}ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from ç•³ã¿è¾¼ã¿.model import NoiseDetectionAndReconstructionModel\n",
    "from eval import evaluate_noise_detection_reconstruction_model\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\n",
    "NUM_INTERVALS = 30\n",
    "POINTS_PER_INTERVAL = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "test_noisy = torch.FloatTensor(dataset['test']['noisy_data'])\n",
    "test_original = torch.FloatTensor(dataset['test']['original_data'])\n",
    "test_labels = torch.LongTensor(dataset['test']['labels'])\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹\n",
    "class NoiseDetectionReconstructionDataset:\n",
    "    def __init__(self, noisy_data, original_data, labels):\n",
    "        self.noisy_data = noisy_data\n",
    "        self.original_data = original_data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.noisy_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'noisy_psd': self.noisy_data[idx],\n",
    "            'original_psd': self.original_data[idx],\n",
    "            'noise_intervals': self.labels[idx]\n",
    "        }\n",
    "\n",
    "test_dataset = NoiseDetectionReconstructionDataset(test_noisy, test_original, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨é‡ã¿ã®èª­ã¿è¾¼ã¿\n",
    "model = NoiseDetectionAndReconstructionModel(\n",
    "    num_intervals=NUM_INTERVALS,\n",
    "    points_per_interval=POINTS_PER_INTERVAL\n",
    ").to(device)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"âœ“ ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {model_path}\")\n",
    "    if 'val_accuracy' in checkpoint:\n",
    "        print(f\"  ä¿å­˜æ™‚ã®æ¤œè¨¼ç²¾åº¦: {checkpoint['val_accuracy']:.2f}%\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}\")\n",
    "    print(\"å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "print(\"\\nâœ… è©•ä¾¡ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è©•ä¾¡ã‚’å®Ÿè¡Œ\n",
    "print(\"\\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...\")\n",
    "results = evaluate_noise_detection_reconstruction_model(\n",
    "    model, test_loader, device=device, num_intervals=NUM_INTERVALS, points_per_interval=POINTS_PER_INTERVAL\n",
    ")\n",
    "\n",
    "print(f\"\\n=== è©•ä¾¡çµæœ ===\")\n",
    "print(f\"  ãƒã‚¹ã‚¯äºˆæ¸¬ç²¾åº¦: {results['mask_accuracy']:.4f}\")\n",
    "print(f\"  ãƒã‚¹ã‚¯äºˆæ¸¬ Precision: {results['mask_precision']:.4f}\")\n",
    "print(f\"  ãƒã‚¹ã‚¯äºˆæ¸¬ Recall: {results['mask_recall']:.4f}\")\n",
    "print(f\"  ãƒã‚¹ã‚¯äºˆæ¸¬ F1-score: {results['mask_f1_score']:.4f}\")\n",
    "if results['loss'] != float('inf'):\n",
    "    print(f\"  å¾©å…ƒæå¤±ï¼ˆMSEæå¤±ã€lossã‚­ãƒ¼ï¼‰: {results['loss']:.6f}\")\n",
    "    if results['reconstruction_accuracy'] is not None:\n",
    "        print(f\"  å¾©å…ƒç²¾åº¦: {results['reconstruction_accuracy']:.2f}%\")\n",
    "else:\n",
    "    print(f\"  å¾©å…ƒæå¤±: è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒã‚¹ã‚¯äºˆæ¸¬ãŒæ­£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰\")\n",
    "print(f\"  ãƒã‚¹ã‚¯äºˆæ¸¬ãŒæ­£ã—ã„ã‚µãƒ³ãƒ—ãƒ«æ•°: {results['num_correct_masks']} / {len(results['labels'])}\")\n",
    "print(f\"\\n  â€» å¾©å…ƒæå¤±ï¼ˆlossã‚­ãƒ¼ï¼‰: {results['loss']:.6f} - æœ€ã‚‚é‡è¦ãªæŒ‡æ¨™\")\n",
    "if results['reconstruction_accuracy'] is not None:\n",
    "    print(f\"  â€» å¾©å…ƒç²¾åº¦: {results['reconstruction_accuracy']:.2f}% - å¾©å…ƒã§ããŸå‰²åˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–ï¼ˆå¾©å…ƒæå¤±ã®ã¿ï¼‰\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# å¾©å…ƒæå¤±\n",
    "ax.plot(train_reconstruction_losses, label='Train Recon Loss', color='blue', linewidth=2)\n",
    "ax.plot(val_reconstruction_losses, label='Val Recon Loss', color='red', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Reconstruction Loss', fontsize=12)\n",
    "ax.set_title('Reconstruction Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"å­¦ç¿’æ›²ç·šã‚’ 'training_curves.png' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "if best_model_state is not None:\n",
    "    torch.save({\n",
    "        'model_state_dict': best_model_state,\n",
    "        'val_accuracy': best_val_acc,\n",
    "        'num_intervals': NUM_INTERVALS,\n",
    "        'points_per_interval': POINTS_PER_INTERVAL,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }, 'noise_detection_reconstruction_model.pth')\n",
    "    print(\"ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ 'noise_detection_reconstruction_model.pth' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"è­¦å‘Š: ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š\n",
    "- `noise_detection_reconstruction_model.pth` (å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«)\n",
    "- `training_curves.png` (å­¦ç¿’æ›²ç·š)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "from google.colab import files\n",
    "\n",
    "files.download('noise_detection_reconstruction_model.pth')\n",
    "files.download('training_curves.png')\n",
    "print(\"âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
