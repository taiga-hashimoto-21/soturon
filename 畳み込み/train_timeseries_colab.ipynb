{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”¨ãƒã‚¤ã‚ºæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆGoogle Colabç‰ˆï¼‰\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸ30ã‚¯ãƒ©ã‚¹åˆ†é¡ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºæ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚\n",
        "\n",
        "## ğŸ“‹ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †\n",
        "\n",
        "1. **GPUã‚’æœ‰åŠ¹åŒ–**: ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  â†’ ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ â†’ GPUï¼ˆT4ï¼‰ã‚’é¸æŠ\n",
        "2. **Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ**\n",
        "3. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™**: å…ˆã« `prepare_dataset_timeseries.ipynb` ã‚’å®Ÿè¡Œã—ã¦ `baseline_dataset_timeseries.pickle` ã‚’ç”Ÿæˆ\n",
        "4. å„ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ãã ã•ã„\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install torch torchvision scikit-learn matplotlib -q\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import platform\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
        "\n",
        "# GPUã®ç¢ºèª\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPUå: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®šï¼ˆColabç”¨ï¼‰\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
        "project_root = '/content/drive/MyDrive/soturon'  # soturonãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ«ãƒ¼ãƒˆ\n",
        "baseline_folder = f'{project_root}/ç•³ã¿è¾¼ã¿'  # ç•³ã¿è¾¼ã¿ãƒ•ã‚©ãƒ«ãƒ€\n",
        "\n",
        "if os.path.exists(baseline_folder):\n",
        "    os.chdir(baseline_folder)\n",
        "    sys.path.insert(0, baseline_folder)\n",
        "    sys.path.insert(0, project_root)\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ {baseline_folder} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    print(\"Google Driveã«soturonãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from model import SimpleCNN\n",
        "from utils.evaluate import evaluate_baseline_model, plot_confusion_matrix\n",
        "from train_timeseries import TimeSeriesDataset, train_epoch, validate\n",
        "\n",
        "print(\"âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«æˆåŠŸã—ã¾ã—ãŸï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
        "dataset_path = f'{baseline_folder}/baseline_dataset_timeseries.pickle'\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {dataset_path}\")\n",
        "    \n",
        "    with open(dataset_path, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    \n",
        "    train_data = dataset['train']['data']\n",
        "    train_labels = dataset['train']['labels']\n",
        "    val_data = dataset['val']['data']\n",
        "    val_labels = dataset['val']['labels']\n",
        "    test_data = dataset['test']['data']\n",
        "    test_labels = dataset['test']['labels']\n",
        "    \n",
        "    config = dataset.get('config', {})\n",
        "    print(f\"\\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æƒ…å ±:\")\n",
        "    print(f\"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
        "    print(f\"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(val_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
        "    print(f\"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,}ã‚µãƒ³ãƒ—ãƒ«\")\n",
        "    print(f\"  ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {config.get('data_type', 'unknown')}\")\n",
        "    print(f\"  ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—: {config.get('noise_type', 'unknown')}\")\n",
        "    print(f\"  ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«: {config.get('noise_level', 'unknown')}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_path}\")\n",
        "    print(\"å…ˆã« prepare_dataset_timeseries.ipynb ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 10\n",
        "NUM_CLASSES = 30\n",
        "\n",
        "print(f\"ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\")\n",
        "print(f\"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}\")\n",
        "print(f\"  å­¦ç¿’ç‡: {LEARNING_RATE}\")\n",
        "print(f\"  ã‚¨ãƒãƒƒã‚¯æ•°: {NUM_EPOCHS}\")\n",
        "print(f\"  ã‚¯ãƒ©ã‚¹æ•°: {NUM_CLASSES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. DataLoaderã®ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataLoaderã®ä½œæˆ\n",
        "train_dataset = TimeSeriesDataset(train_data, train_labels)\n",
        "val_dataset = TimeSeriesDataset(val_data, val_labels)\n",
        "test_dataset = TimeSeriesDataset(test_data, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"âœ… DataLoaderã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
        "model = SimpleCNN(num_classes=NUM_CLASSES).to(device)\n",
        "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã‚’ç¢ºèª\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    dummy_input = torch.randn(1, 3000).to(device)\n",
        "    dummy_output = model(dummy_input)\n",
        "    print(f\"  ãƒ€ãƒŸãƒ¼å…¥åŠ›ã®å½¢çŠ¶: {dummy_input.shape}\")\n",
        "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®å½¢çŠ¶: {dummy_output.shape}\")\n",
        "    print(f\"  ãƒ€ãƒŸãƒ¼å‡ºåŠ›ã®ç¯„å›²: [{dummy_output.min().item():.4f}, {dummy_output.max().item():.4f}]\")\n",
        "model.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®è¨­å®š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "print(\"âœ… æå¤±é–¢æ•°ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. å­¦ç¿’ã®å®Ÿè¡Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦ç¿’å±¥æ­´\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "best_val_acc = 0\n",
        "best_model_state = None\n",
        "\n",
        "print(\"å­¦ç¿’é–‹å§‹...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ç·ã‚¨ãƒãƒƒã‚¯æ•°: {NUM_EPOCHS}\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # å­¦ç¿’\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # æ¤œè¨¼\n",
        "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # å­¦ç¿’ç‡ã®èª¿æ•´\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # å±¥æ­´ã®ä¿å­˜\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict().copy()\n",
        "    \n",
        "    # ãƒ­ã‚°å‡ºåŠ›\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        elapsed_time = time.time() - start_time\n",
        "        remaining_epochs = NUM_EPOCHS - (epoch + 1)\n",
        "        estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs if epoch > 0 else 0\n",
        "        \n",
        "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] ({epoch_time:.1f}ç§’)\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "        if epoch > 0:\n",
        "            print(f\"  çµŒéæ™‚é–“: {elapsed_time/60:.1f}åˆ†, æ®‹ã‚Šè¦‹ç©ã‚‚ã‚Š: {estimated_remaining/60:.1f}åˆ†\")\n",
        "        print()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"å­¦ç¿’å®Œäº†ï¼ç·æ™‚é–“: {total_time/60:.1f}åˆ† ({total_time/3600:.2f}æ™‚é–“)\")\n",
        "print(f\"ãƒ™ã‚¹ãƒˆæ¤œè¨¼ç²¾åº¦: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡\n",
        "print(\"\\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...\")\n",
        "test_loss, test_acc, test_predictions, test_labels_list = validate(\n",
        "    model, test_loader, criterion, device\n",
        ")\n",
        "\n",
        "print(f\"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_acc:.2f}%\")\n",
        "\n",
        "# è©³ç´°ãªè©•ä¾¡\n",
        "results = evaluate_baseline_model(test_predictions, test_labels_list)\n",
        "print(f\"\\nè©³ç´°ãªè©•ä¾¡çµæœ:\")\n",
        "print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {results['precision']:.4f}\")\n",
        "print(f\"  Recall: {results['recall']:.4f}\")\n",
        "print(f\"  F1-score: {results['f1_score']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. çµæœã®å¯è¦–åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–\n",
        "plot_confusion_matrix(\n",
        "    results['confusion_matrix'],\n",
        "    title='æ··åŒè¡Œåˆ—ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰',\n",
        "    save_path=None  # Colabã§ã¯è¡¨ç¤ºã®ã¿\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(train_losses, label='è¨“ç·´æå¤±', color='blue')\n",
        "axes[0].plot(val_losses, label='æ¤œè¨¼æå¤±', color='red')\n",
        "axes[0].set_xlabel('ã‚¨ãƒãƒƒã‚¯')\n",
        "axes[0].set_ylabel('æå¤±')\n",
        "axes[0].set_title('å­¦ç¿’æ›²ç·šï¼ˆæå¤±ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(train_accuracies, label='è¨“ç·´ç²¾åº¦', color='blue')\n",
        "axes[1].plot(val_accuracies, label='æ¤œè¨¼ç²¾åº¦', color='red')\n",
        "axes[1].set_xlabel('ã‚¨ãƒãƒƒã‚¯')\n",
        "axes[1].set_ylabel('ç²¾åº¦ (%)')\n",
        "axes[1].set_title('å­¦ç¿’æ›²ç·šï¼ˆç²¾åº¦ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã‚’Google Driveã«ä¿å­˜\n",
        "model_save_path = f'{baseline_folder}/baseline_model_timeseries.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'test_accuracy': test_acc,\n",
        "    'results': results,\n",
        "    'config': config,\n",
        "    'train_losses': train_losses,\n",
        "    'train_accuracies': train_accuracies,\n",
        "    'val_losses': val_losses,\n",
        "    'val_accuracies': val_accuracies,\n",
        "}, model_save_path)\n",
        "\n",
        "print(f\"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_save_path}\")\n",
        "print(f\"   ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_acc:.2f}%\")\n",
        "print(f\"   Accuracy: {results['accuracy']:.4f}\")\n",
        "print(f\"   F1-score: {results['f1_score']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
