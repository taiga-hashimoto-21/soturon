{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•\n",
        "\n",
        "## Google Driveã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "\n",
        "Google Driveã«`soturon`ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãã®ã¾ã¾ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "**ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ **:\n",
        "```\n",
        "Google Drive/\n",
        "â””â”€â”€ MyDrive/\n",
        "    â””â”€â”€ soturon/\n",
        "        â”œâ”€â”€ è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/\n",
        "        â”‚   â””â”€â”€ task/\n",
        "        â”‚       â”œâ”€â”€ __init__.py\n",
        "        â”‚       â”œâ”€â”€ dataset.py\n",
        "        â”‚       â”œâ”€â”€ model.py\n",
        "        â”‚       â””â”€â”€ train.py\n",
        "        â”œâ”€â”€ ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)/\n",
        "        â”‚   â”œâ”€â”€ __init__.py\n",
        "        â”‚   â”œâ”€â”€ add_noise.py\n",
        "        â”‚   â”œâ”€â”€ power_supply_noise.py\n",
        "        â”‚   â”œâ”€â”€ interference_noise.py\n",
        "        â”‚   â””â”€â”€ clock_leakage_noise.py\n",
        "        â”œâ”€â”€ eval.py\n",
        "        â””â”€â”€ data_lowF_noise.pickle\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ã‚¿ã‚¹ã‚¯4: ãƒã‚¹ã‚¯äºˆæ¸¬ + æ­£å‰‡åŒ–é …\n",
        "\n",
        "è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºæ¤œå‡º\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç’°å¢ƒè¨­å®š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ã“ã®ã‚»ãƒ«ã¯ä¸è¦ï¼ˆã‚»ãƒ«1ã§ãƒã‚¦ãƒ³ãƒˆæ¸ˆã¿ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Google Driveã«soturonãƒ•ã‚©ãƒ«ãƒ€ã‚’ãã®ã¾ã¾ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "project_root = '/content/drive/MyDrive/soturon'  # soturonãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ«ãƒ¼ãƒˆ\n",
        "ssl_folder = f'{project_root}/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’'  # è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ãƒ•ã‚©ãƒ«ãƒ€\n",
        "\n",
        "if os.path.exists(ssl_folder):\n",
        "    os.chdir(ssl_folder)\n",
        "    sys.path.insert(0, ssl_folder)\n",
        "    sys.path.insert(0, project_root)  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚‚è¿½åŠ \n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ {ssl_folder} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    print(\"Google Driveã«soturonãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
        "pickle_path = '/content/drive/MyDrive/soturon/data_lowF_noise.pickle'\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
        "import os\n",
        "if os.path.exists(pickle_path):\n",
        "    print(f\"âœ… Data file found: {pickle_path}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Data file not found at {pickle_path}\")\n",
        "    print(\"Google Driveã®soturonãƒ•ã‚©ãƒ«ãƒ€ã«data_lowF_noise.pickleã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®šï¼ˆé‡è¦ï¼ï¼‰\n",
        "# Google Driveã«soturonãƒ•ã‚©ãƒ«ãƒ€ã‚’ãã®ã¾ã¾ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "project_root = '/content/drive/MyDrive/soturon'  # soturonãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ«ãƒ¼ãƒˆ\n",
        "ssl_folder = f'{project_root}/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’'  # è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ãƒ•ã‚©ãƒ«ãƒ€\n",
        "\n",
        "if os.path.exists(ssl_folder):\n",
        "    os.chdir(ssl_folder)\n",
        "    sys.path.insert(0, ssl_folder)\n",
        "    sys.path.insert(0, project_root)  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚‚è¿½åŠ ï¼ˆeval.pyã‚„ãƒã‚¤ã‚ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç”¨ï¼‰\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ {ssl_folder} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    print(\"Google Driveã«soturonãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "# å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèª\n",
        "required_files = [\n",
        "    f'{ssl_folder}/task/__init__.py',\n",
        "    f'{ssl_folder}/task/dataset.py',\n",
        "    f'{ssl_folder}/task/model.py',\n",
        "    f'{ssl_folder}/task/train.py',\n",
        "    f'{project_root}/ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)/__init__.py',\n",
        "    f'{project_root}/ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)/add_noise.py',\n",
        "    f'{project_root}/ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)/power_supply_noise.py',\n",
        "    f'{project_root}/ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)/interference_noise.py',\n",
        "    f'{project_root}/ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)/clock_leakage_noise.py',\n",
        "    f'{project_root}/eval.py',\n",
        "    f'{project_root}/data_lowF_noise.pickle'\n",
        "]\n",
        "\n",
        "missing_files = []\n",
        "for file in required_files:\n",
        "    if not os.path.exists(file):\n",
        "        # ãƒ‘ã‚¹ã‚’çŸ­ç¸®ã—ã¦è¡¨ç¤º\n",
        "        if file.startswith(ssl_folder):\n",
        "            missing_files.append(file.replace(ssl_folder + '/', 'è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/'))\n",
        "        elif file.startswith(project_root):\n",
        "            missing_files.append(file.replace(project_root + '/', ''))\n",
        "\n",
        "if missing_files:\n",
        "    print(\"âš ï¸ ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:\")\n",
        "    for file in missing_files:\n",
        "        print(f\"  - {file}\")\n",
        "    print(f\"\\nğŸ“ Google Driveã® {project_root} ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n",
        "    print(\"   ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ :\")\n",
        "    print(\"   soturon/\")\n",
        "    print(\"   â”œâ”€â”€ è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/\")\n",
        "    print(\"   â”‚   â””â”€â”€ task/\")\n",
        "    print(\"   â”œâ”€â”€ ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)/\")\n",
        "    print(\"   â”œâ”€â”€ eval.py\")\n",
        "    print(\"   â””â”€â”€ data_lowF_noise.pickle\")\n",
        "else:\n",
        "    print(\"âœ… ã™ã¹ã¦ã®å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼\")\n",
        "\n",
        "# ãƒã‚¤ã‚ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ­£ã—ãç™»éŒ²ï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è§£æ±ºã™ã‚‹ãŸã‚ï¼‰\n",
        "print(\"\\nnoiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’sys.modulesã«ç™»éŒ²ä¸­...\")\n",
        "noise_folder_path = os.path.join(project_root, 'ãƒã‚¤ã‚ºã®ä»˜ä¸(å…±é€š)')\n",
        "\n",
        "# æ—¢å­˜ã®noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‰Šé™¤ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼‰\n",
        "if 'noise' in sys.modules:\n",
        "    del sys.modules['noise']\n",
        "if 'noise.add_noise' in sys.modules:\n",
        "    del sys.modules['noise.add_noise']\n",
        "\n",
        "# __pycache__ã‚’ã‚¯ãƒªã‚¢ï¼ˆå¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ï¼‰\n",
        "import shutil\n",
        "pycache_path = os.path.join(noise_folder_path, '__pycache__')\n",
        "if os.path.exists(pycache_path):\n",
        "    try:\n",
        "        shutil.rmtree(pycache_path)\n",
        "        print(f\"å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤: {pycache_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‰Šé™¤ã«å¤±æ•—ï¼ˆç„¡è¦–ã—ã¾ã™ï¼‰: {e}\")\n",
        "\n",
        "# __init__.pyã®å†…å®¹ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£\n",
        "init_path = os.path.join(noise_folder_path, '__init__.py')\n",
        "print(f\"  __init__.pyã®ãƒ‘ã‚¹: {init_path}\")\n",
        "\n",
        "# Google Colabä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤ã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ­£ã—ã„å†…å®¹ã‚’æ›¸ãè¾¼ã‚€\n",
        "correct_init_content = '''\"\"\"\n",
        "æ¸¬å®šç³»ç”±æ¥ã®ãƒã‚¤ã‚ºã‚’ä»˜ä¸ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
        "\"\"\"\n",
        "\n",
        "from .power_supply_noise import add_power_supply_noise\n",
        "from .interference_noise import add_interference_noise\n",
        "from .clock_leakage_noise import add_clock_leakage_noise\n",
        "from .add_noise import add_noise_to_interval\n",
        "\n",
        "__all__ = [\n",
        "    'add_power_supply_noise',\n",
        "    'add_interference_noise',\n",
        "    'add_clock_leakage_noise',\n",
        "    'add_noise_to_interval'\n",
        "]\n",
        "'''\n",
        "\n",
        "# __init__.pyã®å†…å®¹ã‚’ç¢ºèª\n",
        "try:\n",
        "    with open(init_path, 'r', encoding='utf-8') as f:\n",
        "        current_content = f.read()\n",
        "    \n",
        "    # å¤ã„å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æ›¸ãæ›ãˆã‚‹\n",
        "    if 'frequency_band_noise' in current_content or 'localized_spike_noise' in current_content:\n",
        "        print(\"  å¤ã„__init__.pyã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚æ­£ã—ã„å†…å®¹ã«æ›¸ãæ›ãˆã¾ã™...\")\n",
        "        with open(init_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(correct_init_content)\n",
        "        print(\"  âœ“ __init__.pyã‚’æ›´æ–°ã—ã¾ã—ãŸ\")\n",
        "    else:\n",
        "        print(\"  âœ“ __init__.pyã¯æ—¢ã«æ­£ã—ã„å†…å®¹ã§ã™\")\n",
        "except Exception as e:\n",
        "    print(f\"  âš  __init__.pyã®ç¢ºèªã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶šè¡Œã—ã¾ã™: {e}\")\n",
        "\n",
        "# ãƒã‚¤ã‚ºãƒ•ã‚©ãƒ«ãƒ€ã‚’sys.pathã«è¿½åŠ ï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è§£æ±ºã™ã‚‹ãŸã‚ï¼‰\n",
        "if noise_folder_path not in sys.path:\n",
        "    sys.path.insert(0, noise_folder_path)\n",
        "\n",
        "import importlib.util\n",
        "\n",
        "# noiseãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®__init__.pyã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "init_spec = importlib.util.spec_from_file_location(\"noise\", init_path)\n",
        "init_module = importlib.util.module_from_spec(init_spec)\n",
        "sys.modules['noise'] = init_module\n",
        "# __file__ã¨__path__ã‚’è¨­å®šï¼ˆç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®ãŸã‚ï¼‰\n",
        "init_module.__file__ = init_path\n",
        "init_module.__path__ = [noise_folder_path]\n",
        "init_spec.loader.exec_module(init_module)\n",
        "\n",
        "# add_noise.pyã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "add_noise_path = os.path.join(noise_folder_path, 'add_noise.py')\n",
        "add_noise_spec = importlib.util.spec_from_file_location(\"noise.add_noise\", add_noise_path)\n",
        "add_noise_module = importlib.util.module_from_spec(add_noise_spec)\n",
        "sys.modules['noise.add_noise'] = add_noise_module\n",
        "add_noise_module.__file__ = add_noise_path\n",
        "add_noise_spec.loader.exec_module(add_noise_module)\n",
        "\n",
        "print(\"âœ“ noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’sys.modulesã«ç™»éŒ²ã—ã¾ã—ãŸ\")\n",
        "print(f\"  åˆ©ç”¨å¯èƒ½ãªé–¢æ•°: {init_module.__all__}\")\n",
        "\n",
        "# add_noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç›´æ¥ç™»éŒ²ï¼ˆtask/dataset.pyãŒ`from add_noise import`ã‚’ä½¿ã†ãŸã‚ï¼‰\n",
        "sys.modules['add_noise'] = add_noise_module\n",
        "print(\"âœ“ add_noiseãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç›´æ¥ç™»éŒ²ã—ã¾ã—ãŸ\")\n",
        "\n",
        "# ã‚¿ã‚¹ã‚¯4ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "try:\n",
        "    from task.dataset import Task4Dataset\n",
        "    from task.model import Task4BERT\n",
        "    from task.train import train_task4\n",
        "    import eval\n",
        "    print(\"âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«æˆåŠŸã—ã¾ã—ãŸï¼\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
        "    print(\"   ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ãŸå ´åˆï¼‰\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ãŸå¾Œã¯ã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "modules_to_reload = [\n",
        "    'task.dataset',\n",
        "    'task.model',\n",
        "    'task.train',\n",
        "    'eval'\n",
        "]\n",
        "\n",
        "for module_name in modules_to_reload:\n",
        "    if module_name in sys.modules:\n",
        "        importlib.reload(sys.modules[module_name])\n",
        "        print(f\"âœ“ {module_name}ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ\")\n",
        "\n",
        "# å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå†èª­ã¿è¾¼ã¿å¾Œï¼‰\n",
        "from task.dataset import Task4Dataset\n",
        "from task.model import Task4BERT\n",
        "from task.train import train_task4\n",
        "import eval\n",
        "\n",
        "print(\"\\nâœ… ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿å®Œäº†\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. å­¦ç¿’ã®å®Ÿè¡Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒã‚¤ã‚¹ã®ç¢ºèª\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¦æœ€åˆã‹ã‚‰å­¦ç¿’ã™ã‚‹å ´åˆï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè¡Œï¼‰\n",
        "import os\n",
        "checkpoint_path = \"/content/drive/MyDrive/soturon/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/task4_output/checkpoint.pth\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"æ—¢å­˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚å‰Šé™¤ã—ã¾ã™...\")\n",
        "    os.remove(checkpoint_path)\n",
        "    print(\"âœ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚æœ€åˆã‹ã‚‰å­¦ç¿’ã—ã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚æœ€åˆã‹ã‚‰å­¦ç¿’ã—ã¾ã™ã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦ç¿’å‰ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ï¼ˆå¤ã„ã‚³ãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import importlib\n",
        "import glob\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ä¸­...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
        "project_root = '/content/drive/MyDrive/soturon'\n",
        "ssl_folder = f'{project_root}/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’'\n",
        "\n",
        "if os.path.exists(ssl_folder):\n",
        "    os.chdir(ssl_folder)\n",
        "    sys.path.insert(0, ssl_folder)\n",
        "    sys.path.insert(0, project_root)\n",
        "    print(f\"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ {ssl_folder} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "# 1. __pycache__ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤\n",
        "print(\"\\n1. __pycache__ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ä¸­...\")\n",
        "pycache_dirs = []\n",
        "for root, dirs, files in os.walk(ssl_folder):\n",
        "    if '__pycache__' in dirs:\n",
        "        pycache_path = os.path.join(root, '__pycache__')\n",
        "        pycache_dirs.append(pycache_path)\n",
        "\n",
        "for pycache_dir in pycache_dirs:\n",
        "    try:\n",
        "        shutil.rmtree(pycache_dir)\n",
        "        print(f\"  âœ“ å‰Šé™¤: {pycache_dir.replace(ssl_folder, 'è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš  å‰Šé™¤å¤±æ•—: {pycache_dir.replace(ssl_folder, 'è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’')} - {e}\")\n",
        "\n",
        "# 2. .pycãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤\n",
        "print(\"\\n2. .pycãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ä¸­...\")\n",
        "pyc_files = glob.glob(os.path.join(ssl_folder, '**', '*.pyc'), recursive=True)\n",
        "for pyc_file in pyc_files:\n",
        "    try:\n",
        "        os.remove(pyc_file)\n",
        "        print(f\"  âœ“ å‰Šé™¤: {pyc_file.replace(ssl_folder, 'è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš  å‰Šé™¤å¤±æ•—: {pyc_file.replace(ssl_folder, 'è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’')} - {e}\")\n",
        "\n",
        "# 3. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’sys.modulesã‹ã‚‰å‰Šé™¤ï¼ˆã‚ˆã‚Šå¾¹åº•çš„ã«ï¼‰\n",
        "print(\"\\n3. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’sys.modulesã‹ã‚‰å‰Šé™¤ä¸­...\")\n",
        "# taskã§å§‹ã¾ã‚‹å…¨ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‰Šé™¤\n",
        "modules_to_remove = [name for name in list(sys.modules.keys()) if name.startswith('task')]\n",
        "for module_name in modules_to_remove:\n",
        "    try:\n",
        "        del sys.modules[module_name]\n",
        "        print(f\"  âœ“ å‰Šé™¤: {module_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš  å‰Šé™¤å¤±æ•—: {module_name} - {e}\")\n",
        "\n",
        "# 4. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦reload\n",
        "print(\"\\n4. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦reloadä¸­...\")\n",
        "try:\n",
        "    # ã¾ãšã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "    import task\n",
        "    import task.dataset\n",
        "    import task.model\n",
        "    import task.train\n",
        "    \n",
        "    # æ¬¡ã«reloadï¼ˆã“ã‚ŒãŒé‡è¦ï¼ï¼‰\n",
        "    importlib.reload(task)\n",
        "    importlib.reload(task.dataset)\n",
        "    importlib.reload(task.model)\n",
        "    importlib.reload(task.train)\n",
        "    \n",
        "    # æœ€å¾Œã«å¿…è¦ãªé–¢æ•°/ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "    from task.dataset import Task4Dataset\n",
        "    from task.model import Task4BERT\n",
        "    from task.train import train_task4\n",
        "    \n",
        "    print(\"  âœ“ task.dataset ã‚’å†èª­ã¿è¾¼ã¿\")\n",
        "    print(\"  âœ“ task.model ã‚’å†èª­ã¿è¾¼ã¿\")\n",
        "    print(\"  âœ“ task.train ã‚’å†èª­ã¿è¾¼ã¿\")\n",
        "    \n",
        "    # ç¢ºèª: compute_lossã¨train_task4ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "    import inspect\n",
        "    \n",
        "    # compute_lossé–¢æ•°ã®ç¢ºèª\n",
        "    compute_loss_source = inspect.getsource(task.train.compute_loss)\n",
        "    if '100.0 * noise_penalty' in compute_loss_source:\n",
        "        print(\"  âœ… compute_loss: æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ï¼ˆpenalty_weight=100.0ï¼‰ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™\")\n",
        "    elif '20.0 * noise_penalty' in compute_loss_source:\n",
        "        print(\"  âš ï¸ compute_loss: å¤ã„ã‚³ãƒ¼ãƒ‰ï¼ˆpenalty_weight=20.0ï¼‰ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™\")\n",
        "    else:\n",
        "        print(\"  âš ï¸ compute_loss: ã‚³ãƒ¼ãƒ‰ã®ç¢ºèªãŒã§ãã¾ã›ã‚“ã§ã—ãŸ\")\n",
        "    \n",
        "    # train_task4é–¢æ•°ã®lambda_regãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç¢ºèª\n",
        "    train_task4_sig = inspect.signature(task.train.train_task4)\n",
        "    lambda_reg_default = train_task4_sig.parameters['lambda_reg'].default\n",
        "    print(f\"  train_task4ã®lambda_regãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: {lambda_reg_default}\")\n",
        "    \n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "    train_py_path = os.path.join(ssl_folder, 'task', 'train.py')\n",
        "    if os.path.exists(train_py_path):\n",
        "        import time\n",
        "        mtime = os.path.getmtime(train_py_path)\n",
        "        mtime_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(mtime))\n",
        "        print(f\"  train.pyã®æ›´æ–°æ—¥æ™‚: {mtime_str}\")\n",
        "        \n",
        "        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªï¼ˆpenalty_weightã®å€¤ã‚’ç›´æ¥ç¢ºèªï¼‰\n",
        "        with open(train_py_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "            if '100.0 * noise_penalty' in content:\n",
        "                print(\"  âœ… train.pyãƒ•ã‚¡ã‚¤ãƒ«ã«æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ï¼ˆpenalty_weight=100.0ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™\")\n",
        "            elif '20.0 * noise_penalty' in content:\n",
        "                print(\"  âš ï¸ train.pyãƒ•ã‚¡ã‚¤ãƒ«ã«å¤ã„ã‚³ãƒ¼ãƒ‰ï¼ˆpenalty_weight=20.0ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™\")\n",
        "            else:\n",
        "                print(\"  âš ï¸ train.pyãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸ\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"  âš  ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ã¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†èª­ã¿è¾¼ã¿å®Œäº†\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\næ¬¡ã®ã‚»ãƒ«ï¼ˆCell 15ï¼‰ã§å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦ç¿’ã‚’å®Ÿè¡Œ\n",
        "# æœ€åˆã‹ã‚‰å­¦ç¿’ã™ã‚‹å ´åˆã¯ resume=False ã«å¤‰æ›´ã—ã¦ãã ã•ã„\n",
        "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã™ã‚‹å ´åˆã¯ resume=True ã®ã¾ã¾ã«ã—ã¦ãã ã•ã„\n",
        "\n",
        "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å†èª­ã¿è¾¼ã¿ï¼ˆç¢ºå®Ÿã«æœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ï¼‰\n",
        "import importlib\n",
        "import sys\n",
        "\n",
        "# task.trainãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿\n",
        "if 'task.train' in sys.modules:\n",
        "    importlib.reload(sys.modules['task.train'])\n",
        "\n",
        "# train_task4é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆreloadå¾Œï¼‰\n",
        "from task.train import train_task4\n",
        "\n",
        "# ç¢ºèª: lambda_regã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "import inspect\n",
        "sig = inspect.signature(train_task4)\n",
        "lambda_reg_default = sig.parameters['lambda_reg'].default\n",
        "print(f\"ç¢ºèª: train_task4ã®lambda_regãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤: {lambda_reg_default}\")\n",
        "if lambda_reg_default == 10.0:\n",
        "    print(\"  âš ï¸ æ³¨æ„: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒ10.0ã§ã™ãŒã€å¼•æ•°ã§æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦ãã ã•ã„\")\n",
        "\n",
        "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ•°ã«ä¿å­˜ï¼ˆå¾Œã§ã‚°ãƒ©ãƒ•è¡¨ç¤ºã§ä½¿ç”¨ï¼‰\n",
        "out_dir = \"/content/drive/MyDrive/soturon/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/task4_output\"\n",
        "\n",
        "model, train_losses, val_losses, best_val_loss, train_recon_losses, val_recon_losses, train_recon_accuracies, val_recon_accuracies = train_task4(\n",
        "    pickle_path=pickle_path,\n",
        "    batch_size=4,  # ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚8â†’4ã«å¤‰æ›´\n",
        "    num_epochs=5,  # ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ã§ç´„30åˆ†ï¼‰\n",
        "    lr=1.5e-3,  # 2e-3 â†’ 1.5e-3ã«å¤‰æ›´ï¼ˆå®‰å®šã—ãŸå­¦ç¿’ï¼‰\n",
        "    val_ratio=0.2,\n",
        "    device=device,\n",
        "    out_dir=out_dir,  # å‡ºåŠ›å…ˆã‚’Google Driveã«\n",
        "    resume=False,  # Falseã«å¤‰æ›´: æœ€åˆã‹ã‚‰å­¦ç¿’ / True: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹\n",
        "    lambda_reg=1.0,  # æ­£å‰‡åŒ–é …ã®é‡ã¿ï¼ˆ1.0ã«å¤‰æ›´ã€ãƒã‚¤ã‚ºåŒºé–“äºˆæ¸¬ç²¾åº¦å‘ä¸Šã®ãŸã‚ï¼‰\n",
        "    lambda_recon=50.0,  # å¾©å…ƒæå¤±ã®é‡ã¿ï¼ˆäºˆæ¸¬ãŒå¤–ã‚ŒãŸå ´åˆã€30.0 â†’ 50.0ã«å¢—åŠ ï¼‰\n",
        "    lambda_recon_correct=100.0,  # äºˆæ¸¬ãŒæ­£ã—ã„å ´åˆã®å¾©å…ƒæå¤±ã®é‡ã¿ï¼ˆäºˆæ¸¬ãŒå¤–ã‚ŒãŸå ´åˆã®lambda_reconã‚ˆã‚Šå¤§ããè¨­å®šï¼‰\n",
        "    lambda_mask=20.0,  # ãƒã‚¹ã‚¯äºˆæ¸¬æå¤±ã®é‡ã¿ï¼ˆæ–°è¦è¿½åŠ ã€äºˆæ¸¬ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã«é‡è¦ï¼‰\n",
        "    lambda_noise_interval=500.0,  # ãƒã‚¤ã‚ºåŒºé–“äºˆæ¸¬æå¤±ã®é‡ã¿ï¼ˆ50.0 â†’ 500.0ã«å¤‰æ›´ã€ãƒã‚¤ã‚ºåŒºé–“äºˆæ¸¬ç²¾åº¦ã‚’å¤§å¹…ã«å‘ä¸Šï¼‰\n",
        "    lambda_ranking=300.0,  # ãƒ©ãƒ³ã‚­ãƒ³ã‚°æå¤±ã®é‡ã¿ï¼ˆ30.0 â†’ 300.0ã«å¤‰æ›´ã€ãƒã‚¤ã‚ºåŒºé–“ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ < æ­£å¸¸åŒºé–“ã®æœ€å°ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å¼·åˆ¶ï¼‰\n",
        "    d_model=64,  # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒï¼ˆ128 â†’ 64ã«å¤‰æ›´ã€ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ï¼‰\n",
        "    n_heads=4,  # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°ï¼ˆ2 â†’ 4ã«å¢—åŠ ï¼‰\n",
        "    num_layers=2,  # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼ˆ3 â†’ 2ã«å¤‰æ›´ã€ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ï¼‰\n",
        "    dim_feedforward=128,  # Feedforwardå±¤ã®æ¬¡å…ƒï¼ˆ256 â†’ 128ã«å¤‰æ›´ã€ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ï¼‰\n",
        "    num_intervals=30,\n",
        "    noise_type='power_supply',  # ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—: 'power_supply', 'interference', 'clock_leakage'ï¼ˆuse_random_noise=Falseã®å ´åˆã®ã¿ä½¿ç”¨ï¼‰\n",
        "    use_random_noise=True,  # 3ç¨®é¡ã®ãƒã‚¤ã‚ºã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰\n",
        "    noise_level=0.3,  # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆ30%ï¼‰\n",
        "    margin=0.01,  # 0.2 â†’ 0.01ã«å¤‰æ›´ï¼ˆå­¦ç¿’ãŒé€²ã¿ã‚„ã™ãã™ã‚‹ï¼‰\n",
        "    ranking_margin=0.01,  # ãƒ©ãƒ³ã‚­ãƒ³ã‚°æå¤±ã®ãƒãƒ¼ã‚¸ãƒ³ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"å­¦ç¿’å®Œäº†ï¼å­¦ç¿’æ›²ç·šã‚’è¡¨ç¤ºã—ã¾ã™\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# å­¦ç¿’ãŒçµ‚ã‚ã£ãŸã‚‰è‡ªå‹•ã§ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "if len(train_recon_losses) > 0:\n",
        "    # 1ã¤ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆï¼ˆå¾©å…ƒæå¤±ã®ã¿ï¼‰\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    \n",
        "    # å¾©å…ƒæå¤±ï¼ˆTrainã®ã¿ï¼‰\n",
        "    ax.plot(train_recon_losses, color='blue', linewidth=2)\n",
        "    print(f\"âœ“ Trainå¾©å…ƒæå¤±ãƒ‡ãƒ¼ã‚¿: {len(train_recon_losses)}ã‚¨ãƒãƒƒã‚¯\")\n",
        "    print(f\"  Trainå¾©å…ƒæå¤±ï¼ˆMSEï¼‰: {train_recon_losses[0]:.6f} â†’ {train_recon_losses[-1]:.6f}\")\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Reconstruction Loss', fontsize=12)\n",
        "    ax.set_title('Reconstruction Loss', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
        "    save_path = f\"{out_dir}/training_curves_recon.png\"\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\nâœ“ å­¦ç¿’æ›²ç·šã‚’ '{save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
        "    \n",
        "    # ç”»åƒã‚’è¡¨ç¤º\n",
        "    plt.show()\n",
        "    print(\"âœ“ ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ\")\n",
        "else:\n",
        "    print(\"âš ï¸ Trainå¾©å…ƒæå¤±ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦ç¿’æ›²ç·šã‚’å¯è¦–åŒ–ï¼ˆå¾©å…ƒæå¤±ã®ã¿ï¼‰\n",
        "# ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãŒåˆ‡ã‚Œã¦ã‚‚å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã€pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# ãƒ‘ã‚¹è¨­å®šï¼ˆCell 16ã§è¨­å®šæ¸ˆã¿ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨ã€ãªã‘ã‚Œã°å†è¨­å®šï¼‰\n",
        "if 'out_dir' not in globals():\n",
        "    out_dir = \"/content/drive/MyDrive/soturon/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/task4_output\"\n",
        "else:\n",
        "    out_dir = globals().get('out_dir', \"/content/drive/MyDrive/soturon/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/task4_output\")\n",
        "\n",
        "# å­¦ç¿’å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€ï¼ˆå¤‰æ•°ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰\n",
        "if 'train_recon_losses' not in globals():\n",
        "    # pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
        "    train_recon_losses_path = os.path.join(out_dir, \"train_recon_losses.pkl\")\n",
        "    if os.path.exists(train_recon_losses_path):\n",
        "        with open(train_recon_losses_path, \"rb\") as f:\n",
        "            train_recon_losses = pickle.load(f)\n",
        "        print(f\"âœ“ pickleãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å­¦ç¿’å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {train_recon_losses_path}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ ã‚¨ãƒ©ãƒ¼: å­¦ç¿’å±¥æ­´ã®å¤‰æ•°ã‚‚pickleãƒ•ã‚¡ã‚¤ãƒ«ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "        print(f\"   pickleãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹: {train_recon_losses_path}\")\n",
        "        print(\"   Cell 16ã§å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "        train_recon_losses = []\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã¨å¯è¦–åŒ–\n",
        "if len(train_recon_losses) > 0:\n",
        "    print(f\"âœ“ Trainå¾©å…ƒæå¤±ãƒ‡ãƒ¼ã‚¿: {len(train_recon_losses)}ã‚¨ãƒãƒƒã‚¯\")\n",
        "    \n",
        "    # 1ã¤ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆï¼ˆå¾©å…ƒæå¤±ã®ã¿ï¼‰\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    \n",
        "    # å¾©å…ƒæå¤±ï¼ˆTrainã®ã¿ï¼‰\n",
        "    ax.plot(train_recon_losses, color='blue', linewidth=2)\n",
        "    print(f\"  Trainå¾©å…ƒæå¤±ï¼ˆMSEï¼‰: {train_recon_losses[0]:.6f} â†’ {train_recon_losses[-1]:.6f}\")\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Reconstruction Loss', fontsize=12)\n",
        "    ax.set_title('Reconstruction Loss', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
        "    save_path = os.path.join(out_dir, \"training_curves_recon.png\")\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\nâœ“ å­¦ç¿’æ›²ç·šã‚’ '{save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
        "    \n",
        "    # ç”»åƒã‚’è¡¨ç¤º\n",
        "    plt.show()\n",
        "    print(\"âœ“ ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ\")\n",
        "else:\n",
        "    print(\"âš ï¸ Trainå¾©å…ƒæå¤±ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
        "    print(\"   Cell 16ã§å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# è©•ä¾¡ç”¨ã‚»ãƒ«ï¼ˆç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½ï¼‰\n",
        "# å¤‰æ•°ãŒæ¶ˆãˆã¦ã‚‚ã€ã“ã®ã‚»ãƒ«ã ã‘å®Ÿè¡Œã™ã‚Œã°è©•ä¾¡ã§ãã¾ã™\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ãƒ‘ã‚¹è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰\n",
        "project_root = '/content/drive/MyDrive/soturon'\n",
        "project_path = f'{project_root}/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’'\n",
        "pickle_path = f'{project_root}/data_lowF_noise.pickle'\n",
        "best_model_path = f'{project_root}/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/task4_output/best_val_model.pth'\n",
        "\n",
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
        "if os.path.exists(project_path):\n",
        "    os.chdir(project_path)\n",
        "    sys.path.insert(0, project_path)\n",
        "    sys.path.insert(0, project_root)\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ {project_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    print(\"Google Driveã«soturonãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ï¼ˆæœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºå®Ÿã«èª­ã¿è¾¼ã‚€ï¼‰\n",
        "modules_to_reload = ['task.dataset', 'task.model', 'task.train', 'eval']\n",
        "for module_name in modules_to_reload:\n",
        "    if module_name in sys.modules:\n",
        "        importlib.reload(sys.modules[module_name])\n",
        "        print(f\"âœ“ {module_name}ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ\")\n",
        "\n",
        "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from task.dataset import Task4Dataset\n",
        "from task.model import Task4BERT\n",
        "import eval\n",
        "\n",
        "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨é‡ã¿ã®èª­ã¿è¾¼ã¿\n",
        "seq_len = 3000\n",
        "model = Task4BERT(\n",
        "    seq_len=seq_len,\n",
        "    d_model=64,\n",
        "    n_heads=2,\n",
        "    num_layers=2,\n",
        "    dim_feedforward=128,\n",
        "    dropout=0.1,\n",
        ").to(device)\n",
        "\n",
        "# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿\n",
        "if os.path.exists(best_model_path):\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print(f\"âœ“ ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {best_model_path}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {best_model_path}\")\n",
        "    print(\"å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "print(\"\\nâœ… è©•ä¾¡ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. è©•ä¾¡ã®å®Ÿè¡Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼ˆç‹¬ç«‹ã—ã¦å®Ÿè¡Œå¯èƒ½ï¼‰\n",
        "\n",
        "# ãƒ‘ã‚¹è¨­å®šï¼ˆCell 18ã§è¨­å®šæ¸ˆã¿ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨ã€ãªã‘ã‚Œã°å†è¨­å®šï¼‰\n",
        "if 'pickle_path' not in globals():\n",
        "    pickle_path = '/content/drive/MyDrive/soturon/data_lowF_noise.pickle'\n",
        "if 'device' not in globals():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if 'model' not in globals():\n",
        "    print(\"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Cell 18ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
        "test_dataset = Task4Dataset(\n",
        "    pickle_path=pickle_path,\n",
        "    num_intervals=30,\n",
        "    noise_type='power_supply',  # å­¦ç¿’æ™‚ã¨åŒã˜ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—ï¼ˆuse_random_noise=Falseã®å ´åˆã®ã¿ä½¿ç”¨ï¼‰\n",
        "    use_random_noise=True,  # å­¦ç¿’æ™‚ã¨åŒã˜è¨­å®š\n",
        "    noise_level=0.3,  # å­¦ç¿’æ™‚ã¨åŒã˜ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«\n",
        "    add_structured_noise_flag=True,  # å…¨ä½“çš„ãªæ§‹é€ åŒ–ãƒã‚¤ã‚ºã‚’ä»˜ä¸ï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜ï¼‰\n",
        ")\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# è©•ä¾¡ã‚’å®Ÿè¡Œ\n",
        "print(\"è©•ä¾¡ã‚’å®Ÿè¡Œä¸­...\")\n",
        "results = eval.evaluate_model(\n",
        "    model=model,\n",
        "    dataloader=test_loader,\n",
        "    method='self_supervised',\n",
        "    device=device,\n",
        "    num_intervals=30,\n",
        ")\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "print(\"\\nè©•ä¾¡çµæœ:\")\n",
        "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "print(f\"Precision: {results['precision']:.4f}\")\n",
        "print(f\"Recall: {results['recall']:.4f}\")\n",
        "print(f\"F1-score: {results['f1_score']:.4f}\")\n",
        "if 'loss' in results:\n",
        "    if results['loss'] == float('inf'):\n",
        "        print(f\"å¾©å…ƒæå¤±: è¨ˆç®—ä¸å¯ï¼ˆãƒã‚¤ã‚ºæ¤œçŸ¥ãŒæ­£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰\")\n",
        "    else:\n",
        "        print(f\"å¾©å…ƒæå¤±ï¼ˆlossã‚­ãƒ¼ï¼‰: {results['loss']:.6f} - æœ€ã‚‚é‡è¦ãªæŒ‡æ¨™\")\n",
        "if 'reconstruction_loss' in results and results['reconstruction_loss'] is not None:\n",
        "    print(f\"å¾©å…ƒæå¤±: {results['reconstruction_loss']:.6f}\")\n",
        "if 'reconstruction_accuracy' in results and results['reconstruction_accuracy'] is not None:\n",
        "    print(f\"å¾©å…ƒç²¾åº¦: {results['reconstruction_accuracy']:.2f}%\")\n",
        "if 'roc_auc' in results:\n",
        "    print(f\"ROC-AUC: {results['roc_auc']:.4f}\")\n",
        "if 'attention_diff' in results:\n",
        "    print(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®å·®: {results['attention_diff']:.4f}\")\n",
        "    print(f\"  (æ­£å¸¸åŒºé–“ - ãƒã‚¤ã‚ºåŒºé–“)\")\n",
        "if 'best_threshold' in results and results['best_threshold'] is not None:\n",
        "    print(f\"æœ€é©é–¾å€¤: {results['best_threshold']:.6f}\")\n",
        "else:\n",
        "    print(f\"äºˆæ¸¬æ–¹æ³•: æœ€ã‚‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒä½ã„åŒºé–“ã‚’äºˆæ¸¬ï¼ˆé–¾å€¤ãªã—ï¼‰\")\n",
        "if 'noise_attention_mean' in results:\n",
        "    print(f\"ãƒã‚¤ã‚ºåŒºé–“ã®å¹³å‡ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: {results['noise_attention_mean']:.6f}\")\n",
        "if 'normal_attention_mean' in results:\n",
        "    print(f\"æ­£å¸¸åŒºé–“ã®å¹³å‡ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: {results['normal_attention_mean']:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–\n",
        "eval.plot_confusion_matrix(\n",
        "    results['confusion_matrix'],\n",
        "    title='ã‚¿ã‚¹ã‚¯4: æ··åŒè¡Œåˆ—',\n",
        "    save_path=\"/content/drive/MyDrive/soturon/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/task4_output/confusion_matrix.png\"\n",
        ")\n",
        "print(\"æ··åŒè¡Œåˆ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®åˆ†å¸ƒã‚’å¯è¦–åŒ–\n",
        "if 'attention_weights' in results:\n",
        "    eval.plot_attention_distribution(\n",
        "        results['attention_weights'],\n",
        "        results['labels'],\n",
        "        num_intervals=30,\n",
        "        save_path=\"/content/drive/MyDrive/soturon/è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’/task4_output/attention_distribution.png\"\n",
        "    )\n",
        "    print(\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®åˆ†å¸ƒã‚’ä¿å­˜ã—ã¾ã—ãŸ\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
