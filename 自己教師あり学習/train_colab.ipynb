{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•\n",
        "\n",
        "## Google Driveã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "\n",
        "Google Driveã«`SSL`ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã€ãã®ä¸­ã«ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼š\n",
        "\n",
        "1. **task/** ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆå…¨ä½“ï¼‰\n",
        "   - `__init__.py`\n",
        "   - `dataset.py`\n",
        "   - `model.py`\n",
        "   - `train.py`\n",
        "\n",
        "2. **noise/** ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆå…¨ä½“ï¼‰\n",
        "   - `__init__.py`\n",
        "   - `add_noise.py`\n",
        "   - `frequency_band_noise.py`\n",
        "   - `localized_spike_noise.py`\n",
        "   - `amplitude_dependent_noise.py`\n",
        "\n",
        "3. **eval.py** ãƒ•ã‚¡ã‚¤ãƒ«\n",
        "\n",
        "4. **data_lowF_noise.pickle** ãƒ•ã‚¡ã‚¤ãƒ«\n",
        "\n",
        "**ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ **:\n",
        "```\n",
        "Google Drive/\n",
        "â””â”€â”€ MyDrive/\n",
        "    â””â”€â”€ SSL/\n",
        "        â”œâ”€â”€ task/\n",
        "        â”œâ”€â”€ noise/\n",
        "        â”œâ”€â”€ eval.py\n",
        "        â””â”€â”€ data_lowF_noise.pickle\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ã‚¿ã‚¹ã‚¯4: ãƒã‚¹ã‚¯äºˆæ¸¬ + æ­£å‰‡åŒ–é …\n",
        "\n",
        "è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºæ¤œå‡º\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç’°å¢ƒè¨­å®š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ã“ã®ã‚»ãƒ«ã¯ä¸è¦ï¼ˆã‚»ãƒ«1ã§ãƒã‚¦ãƒ³ãƒˆæ¸ˆã¿ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Google Driveã®SSLãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¨­å®š\n",
        "project_path = '/content/drive/MyDrive/SSL'\n",
        "if os.path.exists(project_path):\n",
        "    os.chdir(project_path)\n",
        "    sys.path.append(project_path)\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ {project_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    print(\"Google Driveã«SSLãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
        "pickle_path = '/content/drive/MyDrive/SSL/data_lowF_noise.pickle'\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
        "import os\n",
        "if os.path.exists(pickle_path):\n",
        "    print(f\"âœ… Data file found: {pickle_path}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Data file not found at {pickle_path}\")\n",
        "    print(\"Google Driveã®SSLãƒ•ã‚©ãƒ«ãƒ€ã«data_lowF_noise.pickleã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n",
        "import os\n",
        "\n",
        "# å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèª\n",
        "base_path = '/content/drive/MyDrive/SSL'\n",
        "required_files = [\n",
        "    f'{base_path}/task/__init__.py',\n",
        "    f'{base_path}/task/dataset.py',\n",
        "    f'{base_path}/task/model.py',\n",
        "    f'{base_path}/task/train.py',\n",
        "    f'{base_path}/noise/__init__.py',\n",
        "    f'{base_path}/noise/add_noise.py',\n",
        "    f'{base_path}/noise/frequency_band_noise.py',\n",
        "    f'{base_path}/noise/localized_spike_noise.py',\n",
        "    f'{base_path}/noise/amplitude_dependent_noise.py',\n",
        "    f'{base_path}/eval.py',\n",
        "    f'{base_path}/data_lowF_noise.pickle'\n",
        "]\n",
        "\n",
        "missing_files = []\n",
        "for file in required_files:\n",
        "    if not os.path.exists(file):\n",
        "        missing_files.append(file.replace(base_path + '/', ''))\n",
        "\n",
        "if missing_files:\n",
        "    print(\"âš ï¸ ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:\")\n",
        "    for file in missing_files:\n",
        "        print(f\"  - {file}\")\n",
        "    print(f\"\\nğŸ“ Google Driveã® {base_path} ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n",
        "    print(\"   ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ :\")\n",
        "    print(\"   SSL/\")\n",
        "    print(\"   â”œâ”€â”€ task/\")\n",
        "    print(\"   â”œâ”€â”€ noise/\")\n",
        "    print(\"   â”œâ”€â”€ eval.py\")\n",
        "    print(\"   â””â”€â”€ data_lowF_noise.pickle\")\n",
        "else:\n",
        "    print(\"âœ… ã™ã¹ã¦ã®å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼\")\n",
        "\n",
        "# ã‚¿ã‚¹ã‚¯4ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "try:\n",
        "    from task.dataset import Task4Dataset\n",
        "    from task.model import Task4BERT\n",
        "    from task.train import train_task4\n",
        "    import eval\n",
        "    print(\"âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«æˆåŠŸã—ã¾ã—ãŸï¼\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
        "    print(\"   ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. å­¦ç¿’ã®å®Ÿè¡Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒã‚¤ã‚¹ã®ç¢ºèª\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¦æœ€åˆã‹ã‚‰å­¦ç¿’ã™ã‚‹å ´åˆï¼ˆå¿…è¦ã«å¿œã˜ã¦å®Ÿè¡Œï¼‰\n",
        "import os\n",
        "checkpoint_path = \"/content/drive/MyDrive/SSL/task4_output/checkpoint.pth\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"æ—¢å­˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚å‰Šé™¤ã—ã¾ã™...\")\n",
        "    os.remove(checkpoint_path)\n",
        "    print(\"âœ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚æœ€åˆã‹ã‚‰å­¦ç¿’ã—ã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚æœ€åˆã‹ã‚‰å­¦ç¿’ã—ã¾ã™ã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å­¦ç¿’ã‚’å®Ÿè¡Œ\n",
        "# æœ€åˆã‹ã‚‰å­¦ç¿’ã™ã‚‹å ´åˆã¯ resume=False ã«å¤‰æ›´ã—ã¦ãã ã•ã„\n",
        "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã™ã‚‹å ´åˆã¯ resume=True ã®ã¾ã¾ã«ã—ã¦ãã ã•ã„\n",
        "model, train_losses, val_losses, best_val_loss = train_task4(\n",
        "    pickle_path=pickle_path,\n",
        "    batch_size=16,  # é€Ÿåº¦å‘ä¸Šã®ãŸã‚8â†’16ã«å¤‰æ›´\n",
        "    num_epochs=10,  # ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆ10ã‚¨ãƒãƒƒã‚¯ã§ç´„1æ™‚é–“ï¼‰\n",
        "    lr=1e-4,\n",
        "    val_ratio=0.2,\n",
        "    device=device,\n",
        "    out_dir=\"/content/drive/MyDrive/SSL/task4_output\",  # å‡ºåŠ›å…ˆã‚’Google Driveã«\n",
        "    resume=False,  # Falseã«å¤‰æ›´: æœ€åˆã‹ã‚‰å­¦ç¿’ / True: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹\n",
        "    lambda_reg=0.5,  # æ­£å‰‡åŒ–é …ã®é‡ã¿ã‚’å¢—ã‚„ã™ï¼ˆ0.1 â†’ 0.5ï¼‰\n",
        "    num_intervals=30,\n",
        "    noise_type='frequency_band',  # ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—: 'frequency_band', 'localized_spike', 'amplitude_dependent'\n",
        "    noise_level=0.3,  # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆ30%ï¼‰\n",
        "    margin=0.2,  # ãƒ©ãƒ³ã‚­ãƒ³ã‚°æå¤±ã®ãƒãƒ¼ã‚¸ãƒ³ã‚’å¢—ã‚„ã™ï¼ˆ0.1 â†’ 0.2ï¼‰\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è©•ä¾¡ã«å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®šï¼ˆé‡è¦ï¼ï¼‰\n",
        "project_path = '/content/drive/MyDrive/SSL'\n",
        "if os.path.exists(project_path):\n",
        "    os.chdir(project_path)\n",
        "    sys.path.insert(0, project_path)\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ {project_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    print(\"Google Driveã«SSLãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from task.dataset import Task4Dataset\n",
        "from task.model import Task4BERT\n",
        "import torch\n",
        "import eval\n",
        "\n",
        "# evalãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ï¼ˆeval.pyã‚’å¤‰æ›´ã—ãŸå ´åˆï¼‰\n",
        "if 'eval' in sys.modules:\n",
        "    importlib.reload(sys.modules['eval'])\n",
        "    print(\"evalãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã—ãŸ\")\n",
        "\n",
        "# ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®šï¼ˆã¾ã è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰\n",
        "if 'device' not in globals():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã®è¨­å®šï¼ˆã¾ã è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰\n",
        "if 'pickle_path' not in globals():\n",
        "    pickle_path = \"/content/drive/MyDrive/SSL/data_lowF_noise.pickle\"\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆã¾ã åˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰\n",
        "if 'model' not in globals() or model is None:\n",
        "    seq_len = 3000\n",
        "    model = Task4BERT(\n",
        "        seq_len=seq_len,\n",
        "        d_model=64,\n",
        "        n_heads=2,\n",
        "        num_layers=2,\n",
        "        dim_feedforward=128,\n",
        "        dropout=0.1,\n",
        "    ).to(device)\n",
        "    print(\"ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. è©•ä¾¡ã®å®Ÿè¡Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
        "test_dataset = Task4Dataset(\n",
        "    pickle_path=pickle_path,\n",
        "    num_intervals=30,\n",
        "    noise_type='frequency_band',  # å­¦ç¿’æ™‚ã¨åŒã˜ãƒã‚¤ã‚ºã‚¿ã‚¤ãƒ—\n",
        "    noise_level=0.3,  # å­¦ç¿’æ™‚ã¨åŒã˜ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«\n",
        "    add_structured_noise_flag=True,  # å…¨ä½“çš„ãªæ§‹é€ åŒ–ãƒã‚¤ã‚ºã‚’ä»˜ä¸ï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜ï¼‰\n",
        ")\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # é€Ÿåº¦å‘ä¸Šã®ãŸã‚8â†’16ã«å¤‰æ›´\n",
        "\n",
        "# æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
        "best_model_path = \"/content/drive/MyDrive/SSL/task4_output/best_val_model.pth\"\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "\n",
        "# è©•ä¾¡ã‚’å®Ÿè¡Œ\n",
        "results = eval.evaluate_model(\n",
        "    model=model,\n",
        "    dataloader=test_loader,\n",
        "    method='self_supervised',\n",
        "    device=device,\n",
        "    num_intervals=30,\n",
        ")\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "print(\"\\nè©•ä¾¡çµæœ:\")\n",
        "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "print(f\"Precision: {results['precision']:.4f}\")\n",
        "print(f\"Recall: {results['recall']:.4f}\")\n",
        "print(f\"F1-score: {results['f1_score']:.4f}\")\n",
        "if 'roc_auc' in results:\n",
        "    print(f\"ROC-AUC: {results['roc_auc']:.4f}\")\n",
        "if 'attention_diff' in results:\n",
        "    print(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®å·®: {results['attention_diff']:.4f}\")\n",
        "    print(f\"  (æ­£å¸¸åŒºé–“ - ãƒã‚¤ã‚ºåŒºé–“)\")\n",
        "if 'best_threshold' in results:\n",
        "    print(f\"æœ€é©é–¾å€¤: {results['best_threshold']:.6f}\")\n",
        "if 'noise_attention_mean' in results:\n",
        "    print(f\"ãƒã‚¤ã‚ºåŒºé–“ã®å¹³å‡ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: {results['noise_attention_mean']:.6f}\")\n",
        "if 'normal_attention_mean' in results:\n",
        "    print(f\"æ­£å¸¸åŒºé–“ã®å¹³å‡ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: {results['normal_attention_mean']:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–\n",
        "eval.plot_confusion_matrix(\n",
        "    results['confusion_matrix'],\n",
        "    title='ã‚¿ã‚¹ã‚¯4: æ··åŒè¡Œåˆ—',\n",
        "    save_path=\"/content/drive/MyDrive/SSL/task4_output/confusion_matrix.png\"\n",
        ")\n",
        "print(\"æ··åŒè¡Œåˆ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®åˆ†å¸ƒã‚’å¯è¦–åŒ–\n",
        "if 'attention_weights' in results:\n",
        "    eval.plot_attention_distribution(\n",
        "        results['attention_weights'],\n",
        "        results['labels'],\n",
        "        num_intervals=30,\n",
        "        save_path=\"/content/drive/MyDrive/SSL/task4_output/attention_distribution.png\"\n",
        "    )\n",
        "    print(\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®åˆ†å¸ƒã‚’ä¿å­˜ã—ã¾ã—ãŸ\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
