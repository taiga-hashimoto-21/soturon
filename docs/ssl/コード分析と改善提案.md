# 共同研究者のコード分析と改善提案

## コードの概要

共同研究者が実装したコードは、**マスク予測タスク**を実装していますが、**タスク4で必要な正則化項がまだ実装されていません**。

### 現在の実装状況

✅ **実装済み**:
- データセットクラス（`NoiseBERTDataset`）
- BERT風モデル（`NoiseBERT`）
- CLSトークンとMASKトークンの使用
- マスク予測タスク（MSE損失）
- 学習ループ

❌ **未実装**:
- アテンションウェイトの取得機能
- ノイズ区間のアテンションを下げる正則化項
- ノイズ区間情報の使用

---

## コードの詳細分析

### 1. NoiseBERTDataset

**良い点**:
- 振動ノイズを付与している
- 「外的ノイズが強くない部分」だけをマスクする工夫がある
- データの正規化を実装している

**問題点**:
- ノイズ区間の情報（`noise_intervals`）を返していない
- タスク4で必要なノイズ区間の位置情報が取得できない

### 2. NoiseBERT

**良い点**:
- CLSトークンとMASKトークンを実装している
- Transformerエンコーダーを使用している
- シンプルで理解しやすい構造

**問題点**:
- アテンションウェイトを取得する機能がない
- 正則化項を計算するために必要な情報が取得できない

### 3. 学習ループ

**良い点**:
- 混合精度学習（AMP）を使用している
- チェックポイント機能がある
- 損失カーブの可視化がある

**問題点**:
- マスク予測損失のみ（正則化項がない）
- ノイズ区間のアテンションを下げる処理がない

---

## 改善提案

### 改善1: アテンションウェイトを取得できるようにする

`NoiseBERT`クラスに、アテンションウェイトを取得する機能を追加する必要があります。

```python
class NoiseBERT(nn.Module):
    def __init__(self, ...):
        # ... 既存のコード ...
        
        # アテンションウェイトを保存するためのフック
        self.attention_weights = None
    
    def forward(self, x, mask_positions, return_attention=False):
        # ... 既存のコード ...
        
        # アテンションウェイトを取得するためにフックを設定
        if return_attention:
            self._register_attention_hooks()
        
        # Transformerエンコーダーを通す
        encoded = self.encoder(x_with_cls)
        
        # アテンションウェイトを取得
        attention_weights = None
        if return_attention:
            attention_weights = self._get_attention_weights(B)
        
        return out, cls_out, attention_weights
    
    def _register_attention_hooks(self):
        """アテンションウェイトを取得するためのフックを登録"""
        self.attention_weights = []
        
        def hook_fn(module, input, output):
            # MultiHeadAttentionの出力からアテンションウェイトを取得
            if hasattr(module, 'attention_weights'):
                self.attention_weights.append(module.attention_weights)
        
        # 各TransformerEncoderLayerのMultiHeadAttentionにフックを登録
        for layer in self.encoder.layers:
            layer.self_attn.register_forward_hook(hook_fn)
    
    def _get_attention_weights(self, batch_size):
        """アテンションウェイトを取得"""
        if self.attention_weights is None or len(self.attention_weights) == 0:
            return None
        # 最後のレイヤーのアテンションウェイトを使用
        return self.attention_weights[-1]
```

**注意**: PyTorchの`TransformerEncoderLayer`はデフォルトでアテンションウェイトを返さないため、カスタム実装が必要な場合があります。

---

### 改善2: データセットにノイズ区間情報を追加

`NoiseBERTDataset`の`__getitem__`メソッドで、ノイズ区間の情報を返すように修正します。

```python
def __getitem__(self, idx):
    # ... 既存のコード ...
    
    # ノイズ区間のインデックスを計算
    # 振動ノイズが強い部分をノイズ区間とする
    noise_intervals = np.where(strong_vib)[0]
    
    # ノイズ区間が空の場合は、ランダムに1つ選ぶ
    if len(noise_intervals) == 0:
        noise_intervals = np.array([np.random.randint(L)])
    
    # ノイズ区間のインデックスを返す（30区間に分割した場合の区間番号）
    num_intervals = 30
    points_per_interval = L // num_intervals
    noise_interval_idx = noise_intervals[0] // points_per_interval
    noise_interval_idx = min(noise_interval_idx, num_intervals - 1)
    
    return {
        "input": input_seq,
        "target": target_seq,
        "mask": mask_positions,
        "freq": f_tensor,
        "noise_interval": torch.tensor(noise_interval_idx, dtype=torch.long),  # 追加
    }
```

---

### 改善3: 損失関数に正則化項を追加

学習ループの損失計算部分を修正します。

```python
def train_noise_bert(...):
    # ... 既存のコード ...
    
    for epoch in range(start_epoch, num_epochs):
        # ----- Train -----
        model.train()
        running_train_loss = 0.0
        running_mask_loss = 0.0
        running_reg_loss = 0.0
        
        for batch in train_loader:
            x = batch["input"].to(device)
            y = batch["target"].to(device)
            m = batch["mask"].to(device)
            noise_intervals = batch["noise_interval"].to(device)  # 追加
            
            optimizer.zero_grad()
            
            with autocast(enabled=(device == "cuda")):
                # アテンションウェイトも取得
                pred, cls_out, attention_weights = model(x, m, return_attention=True)
                
                # 1. マスク予測損失
                if m.any():
                    mask_loss = criterion(pred[m], y[m])
                else:
                    mask_loss = torch.tensor(0.0, device=device)
                
                # 2. 正則化項: ノイズ区間のアテンションを下げる
                reg_loss = torch.tensor(0.0, device=device)
                if attention_weights is not None:
                    # CLSトークンから各区間へのアテンション
                    # attention_weights: (num_heads, B, L+1, L+1)
                    # CLSトークンはインデックス0
                    cls_attention = attention_weights[:, :, 0, 1:]  # (num_heads, B, L)
                    
                    # ノイズ区間のアテンションを取得
                    # 注意: ノイズ区間のインデックスを適切に変換する必要がある
                    # ここでは簡易的に、ノイズ区間の位置を計算
                    batch_size = x.size(0)
                    noise_attention_list = []
                    for i in range(batch_size):
                        noise_idx = noise_intervals[i].item()
                        # 30区間に分割した場合のインデックスに変換
                        # 簡易的に、ノイズ区間の位置を計算
                        noise_pos = noise_idx * (x.size(1) // 30)
                        if noise_pos < cls_attention.shape[2]:
                            noise_attention_list.append(cls_attention[:, i, noise_pos])
                    
                    if len(noise_attention_list) > 0:
                        noise_attention = torch.stack(noise_attention_list, dim=1)  # (num_heads, B)
                        reg_loss = lambda_reg * noise_attention.mean()
                
                # 総損失
                loss = mask_loss + reg_loss
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            running_train_loss += loss.item() * x.size(0)
            running_mask_loss += mask_loss.item() * x.size(0)
            running_reg_loss += reg_loss.item() * x.size(0)
        
        train_loss = running_train_loss / len(train_loader.dataset)
        train_mask_loss = running_mask_loss / len(train_loader.dataset)
        train_reg_loss = running_reg_loss / len(train_loader.dataset)
        
        # ... 既存のコード ...
```

---

## 完全な修正版コード

### 修正1: NoiseBERTクラス（アテンション取得機能追加）

```python
class NoiseBERT(nn.Module):
    def __init__(
        self,
        seq_len: int,
        d_model: int = 64,
        n_heads: int = 2,
        num_layers: int = 2,
        dim_feedforward: int = 128,
        dropout: float = 0.1,
        max_len: int = None,
    ):
        super().__init__()
        # ... 既存のコード ...
        
        # アテンションウェイトを保存するためのリスト
        self.attention_weights_list = []
    
    def forward(self, x, mask_positions, return_attention=False):
        B, L = x.shape
        
        # ... 既存のコード（埋め込み、マスク、位置埋め込み、CLS追加）...
        
        # アテンションウェイトを取得するためにフックを設定
        if return_attention:
            self.attention_weights_list = []
            self._register_attention_hooks()
        
        # Transformer (S, B, E)
        x_with_cls = x_with_cls.transpose(0, 1)
        encoded = self.encoder(x_with_cls)
        
        # アテンションウェイトを取得
        attention_weights = None
        if return_attention and len(self.attention_weights_list) > 0:
            # 最後のレイヤーのアテンションウェイトを使用
            attention_weights = self.attention_weights_list[-1]
            # shape: (num_heads, B, L+1, L+1)
        
        # 系列部分取り出し
        encoded_seq = encoded[1:].transpose(0, 1)  # (B, L, d_model)
        
        # 出力（回帰）
        out = self.output_head(encoded_seq).squeeze(-1)  # (B, L)
        
        # CLS も返す
        cls_out = encoded[0]
        
        if return_attention:
            return out, cls_out, attention_weights
        else:
            return out, cls_out
    
    def _register_attention_hooks(self):
        """アテンションウェイトを取得するためのフックを登録"""
        def hook_fn(module, input, output):
            # TransformerEncoderLayerのself_attnからアテンションを取得
            # 注意: PyTorchの標準実装では直接取得できないため、
            # カスタム実装が必要な場合があります
            pass
        
        # 各レイヤーにフックを登録
        for layer in self.encoder.layers:
            layer.self_attn.register_forward_hook(hook_fn)
```

**注意**: PyTorchの`TransformerEncoderLayer`はデフォルトでアテンションウェイトを返さないため、カスタムの`MultiheadAttention`を実装する必要がある場合があります。

---

### 修正2: カスタムMultiheadAttention（アテンション取得用）

```python
class AttentionWithWeights(nn.Module):
    """アテンションウェイトを取得できるMultiheadAttention"""
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, n_heads, batch_first=False)
        self.attention_weights = None
    
    def forward(self, query, key, value, **kwargs):
        output, attn_weights = self.attention(query, key, value, **kwargs)
        self.attention_weights = attn_weights
        return output
```

---

## 実装の優先順位

### フェーズ1: 基本的な修正（必須）

1. ✅ データセットにノイズ区間情報を追加
2. ✅ 損失関数に正則化項を追加（簡易版）
3. ✅ 学習ループを修正

### フェーズ2: アテンション取得機能（推奨）

1. ⚠️ カスタムMultiheadAttentionを実装
2. ⚠️ アテンションウェイトを取得する機能を追加
3. ⚠️ 正則化項を正確に計算

---

## 簡易的な実装案（アテンション取得なし）

アテンションウェイトの取得が難しい場合は、**簡易的な正則化項**を実装することもできます：

```python
# CLSトークンの表現を使って、ノイズ区間の特徴を下げる
# （アテンションウェイトの代わりに、CLSトークンの表現を使う）

# ノイズ区間の特徴を取得
noise_features = pred[:, noise_intervals]  # 簡易的に予測値を使う

# ノイズ区間の特徴を下げる（小さくする）
reg_loss = lambda_reg * noise_features.abs().mean()
```

---

## まとめ

### 現在の状態
- ✅ マスク予測タスクは実装済み
- ❌ 正則化項が未実装

### 必要な修正
1. データセットにノイズ区間情報を追加
2. アテンションウェイトを取得する機能を追加（または簡易版を実装）
3. 損失関数に正則化項を追加

### 推奨アプローチ
1. **まず簡易版を実装**（アテンション取得なし）
2. **動作確認後、アテンション取得機能を追加**
3. **正則化項の重み（λ）を調整**

この順序で進めることをおすすめします！

