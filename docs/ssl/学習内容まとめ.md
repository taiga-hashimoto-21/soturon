# 研究で学習すべき内容まとめ

## 研究の概要

**目的**: PSD（パワースペクトル密度）データからノイズが載っている区間を検出する

**データ**: 野中さんが作成したPSDデータ（3万2000件）

**ゴール**: 
1. ノイズ検出ができるようになること（第1目標）
2. ノイズを検出して排除したデータで解析した方が良い結果になることを示す（できれば）

---

## 学習すべき主要な技術・概念

### 1. **自己教師学習（Self-Supervised Learning）** ⭐最重要

**なぜ必要か**: 
- 活性化エネルギーのデータは使えないため、入力データだけで学習する必要がある
- ラベルなしで学習できる手法が必要

**学習内容**:
- 自己教師学習の基本概念
- 代表的な手法：
  - **マスクドランゲージモデリング（Masked Language Modeling）**: 一部をマスクして予測する
  - **データ拡張ベースの手法**: 同じデータから異なる変換を施したペアを作り、埋め込み表現を学習
  - **次文予測**: データの連続性を学習
- 損失関数の設計方法

**参考論文・資料**:
- BERT、GPTなどのTransformerベースの自己教師学習モデル
- SimCLR、MoCoなどのコントラスト学習

---

### 2. **Transformer / アテンション機構** ⭐最重要

**なぜ必要か**: 
- ノイズ区間のアテンションウェイトを制御する必要がある
- アテンションウェイトを使ってノイズの存在確率を判定する

**学習内容**:
- Transformerアーキテクチャの基本構造
- **アテンション機構（Attention Mechanism）**:
  - Self-Attention
  - Multi-Head Attention
  - アテンションウェイトの意味と解釈
- Transformerの実装方法（PyTorch/TensorFlow）
- 時系列データへのTransformer適用

**参考資料**:
- "Attention Is All You Need" (Vaswani et al., 2017)
- Transformerの実装チュートリアル

---

### 3. **損失関数と正則化項（Regularization）**

**なぜ必要か**: 
- ノイズ区間のアテンションウェイトを下げるような制約を損失関数に追加する必要がある

**学習内容**:
- **損失関数（Loss Function）**: タスクの評価指標
- **正則化項（Regularization Term）**: モデルへの制約
  - L1正則化、L2正則化
  - カスタム正則化項の設計
- 損失関数と正則化項の違いと使い分け
- アテンションウェイトを損失関数に組み込む方法

**具体例**:
```
総損失 = タスクの損失 + λ × ノイズ区間のアテンションウェイトの合計
```
（λはハイパーパラメータ）

---

### 4. **PSD（パワースペクトル密度）と信号処理**

**なぜ必要か**: 
- 研究で使用するデータがPSD形式

**学習内容**:
- PSDの定義と意味
- フーリエ変換とスペクトル解析
- 時系列データからPSDへの変換
- PSDデータの可視化と解釈
- ノイズの特徴（周波数領域での表現）

---

### 5. **音声認識・時系列データ処理**

**なぜ必要か**: 
- 参考論文が音声認識の分野
- PSDデータは時系列データの一種

**学習内容**:
- 音声認識の基本（参考として）
- 時系列データの前処理
- 時系列データへの深層学習適用
- データ拡張（Data Augmentation）:
  - 時系列データの拡張手法
  - ノイズ付加、時間シフト、スケーリングなど

---

### 6. **深層学習の実装スキル**

**学習内容**:
- **PyTorch** または **TensorFlow** の基本操作
- ニューラルネットワークの実装
- Transformerモデルの実装
- 学習ループの作成
- ハイパーパラメータ調整

---

### 7. **評価指標と実験設計**

**学習内容**:
- ノイズ検出の評価方法
- アテンションウェイトの解釈方法
- 実験の設計と比較方法
- ノイズ排除後のデータ解析結果の評価

---

## 学習の優先順位

### 最優先（すぐに学習すべき）
1. **自己教師学習** - タスク設計の核心
2. **Transformer/アテンション機構** - モデルの基盤

### 高優先度
3. **損失関数と正則化項** - 実装に必要
4. **深層学習の実装スキル** - 実際にコードを書くために必要

### 中優先度
5. **PSDと信号処理** - データ理解のために必要
6. **音声認識・時系列データ処理** - 参考として

### 低優先度（後で学習）
7. **評価指標と実験設計** - 実験段階で必要

---

## 具体的な学習リソース

### 書籍
- 「深層学習」（花書院）
- 「ゼロから作るDeep Learning」シリーズ

### オンラインコース
- Fast.ai
- Coursera: Deep Learning Specialization
- Udacity: Deep Learning Nanodegree

### 論文
- "Attention Is All You Need" (Transformer)
- BERT論文（自己教師学習）
- SimCLR論文（コントラスト学習）

### 実装例
- Hugging Face Transformers
- PyTorch公式チュートリアル
- TensorFlow公式チュートリアル

---

## 次のステップ

1. **自己教師学習の基本を理解する**（1-2週間）
2. **Transformerの実装を理解する**（1-2週間）
3. **簡単な自己教師学習タスクを実装してみる**（2-3週間）
4. **PSDデータを読み込んで可視化する**（1週間）
5. **研究用のタスクを設計する**（継続的に）

---

## 注意点

- **タスク設計が重要**: 自己教師学習のタスクをどう設計するかが研究の成否を分ける
- **アテンションウェイトの解釈**: ノイズ検出にアテンションウェイトを使うため、その意味を理解することが重要
- **実装力**: 理論だけでなく、実際にコードを書けることが必要

